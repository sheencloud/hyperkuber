{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"\u4ecb\u7ecd \u7565 \u76d1\u63a7\u544a\u8b66\u67b6\u6784 \u76ee\u524d\u9c81\u73ed\u4ea7\u54c1\u91c7\u7528Prometheus\u7684\u6574\u4f53\u67b6\u6784\uff0c\u540c\u65f6\u4e5f\u53c2\u7167kingsoft cloud\u7684eagles\u76d1\u63a7\u67b6\u6784\u4fee\u6539\uff0ceagles\u76ee\u524d\u91c7\u7528\u67b6\u6784\u4e3a\uff1a prometheus\u7684\u539f\u67b6\u6784\uff1a \u76ee\u524d\u9c81\u73ed\u67b6\u6784\u589e\u52a0luban_agent\u63a5\u53d7\u539f\u6709\u7684eagles\u7684\u81ea\u5b9a\u4e49\u76d1\u63a7\u811a\u672c\uff0c\u589e\u52a0node_exporter\u7269\u7406\u673a\u76d1\u63a7\uff0c\u589e\u52a0\u544a\u8b66\u5386\u53f2\u5b58\u50a8\u5230elasticsearch\u4ee5\u53ca\u5b9e\u65f6\u544a\u8b66\u5904\u7406\u63a8\u9001\u98de\u4e66\uff0c\u5177\u4f53\u67b6\u6784\u5982\u4e0b\uff1a \u65e5\u5fd7\u67b6\u6784 \u5728\u65e5\u5fd7\u6536\u96c6\u4e2d\uff0c\u90fd\u662f\u4f7f\u7528\u7684filebeat+ELK\u7684\u65e5\u5fd7\u67b6\u6784\u3002\u4f46\u662f\u5982\u679c\u4e1a\u52a1\u6bcf\u5929\u4f1a\u4ea7\u751f\u6d77\u91cf\u7684\u65e5\u5fd7\uff0c\u5c31\u6709\u53ef\u80fd\u5f15\u53d1logstash\u548celasticsearch\u7684\u6027\u80fd\u74f6\u9888\u95ee\u9898\u3002\u56e0\u6b64\u6539\u5584\u8fd9\u4e00\u95ee\u9898\u7684\u65b9\u6cd5\u5c31\u662ffilebeat+kafka+logstash+ELK\uff0c \u4e5f\u5c31\u662f\u5c06\u5b58\u50a8\u4eceelasticsearch\u8f6c\u79fb\u7ed9\u6d88\u606f\u4e2d\u95f4\u4ef6\uff0c\u51cf\u5c11\u6d77\u91cf\u6570\u636e\u5f15\u8d77\u7684\u5b95\u673a\uff0c\u964d\u4f4eelasticsearch\u7684\u538b\u529b\uff0c\u8fd9\u91cc\u7684elasticsearch\u4e3b\u8981\u8fdb\u884c\u6570\u636e\u7684\u5206\u6790\u5904\u7406\uff0c\u7136\u540e\u4ea4\u7ed9kibana\u8fdb\u884c\u754c\u9762\u5c55\u793a CMDB\u67b6\u6784 Dgraph\u7ec4\u4ef6\u5305\u62ec\u4e09\u4e2a\u90e8\u5206\uff1a Zero: \u662f\u96c6\u7fa4\u7684\u6838\u5fc3, \u8d1f\u8d23\u8c03\u5ea6\u96c6\u7fa4\u670d\u52a1\u5668\u548c\u5e73\u8861\u670d\u52a1\u5668\u7ec4\u4e4b\u95f4\u7684\u6570\u636e\uff0c\u7c7b\u6bd4\u4e8eElasticsearch\u7684master\u8282\u70b9\uff1b Alpha: \u4fdd\u5b58\u6570\u636e\u7684 \u8c13\u8bcd \u548c \u7d22\u5f15. \u8c13\u8bcd\u5305\u62ec\u6570\u636e\u7684 \u5c5e\u6027 \u548c\u6570\u636e\u4e4b\u95f4\u7684 \u5173\u7cfb; \u7d22\u5f15\u662f\u4e3a\u4e86\u66f4\u5feb\u7684\u8fdb\u884c\u6570\u636e\u7684\u8fc7\u6ee4\u548c\u67e5\u627e\uff0c\u7c7b\u6bd4\u4e8eElasticsearch\u7684data\u8282\u70b9\uff1b Ratel: dgraph \u7684 UI \u63a5\u53e3, \u53ef\u4ee5\u5728\u6b64\u754c\u9762\u4e0a\u8fdb\u884c\u6570\u636e\u7684 CURD, \u4e5f\u53ef\u4ee5\u4fee\u6539\u6570\u636e\u7684 schema\uff0c\u7c7b\u6bd4\u4e8eElasticsearch\u7684kibana\u89d2\u8272 \u5b89\u88c5\u6b65\u9aa4 \u73af\u5883\u51c6\u5907 \u76ee\u524d\u9c81\u73ed\u7684\u7ec4\u4ef6\uff08\u9664\u76d1\u63a7\u6570\u636e\u91c7\u96c6\u7aef\u7684node_exporter\u4ee5\u53caluban_agent\u662f\u90e8\u7f72\u5728\u7269\u7406\u673a\u670d\u52a1\u5668\u4e0a\u4e4b\u5916\uff0c\u5176\u4ed6\u7ec4\u4ef6\u5747\u90e8\u7f72\u5728kubernetes\u96c6\u7fa4\u4e2d\uff0c\u5b89\u88c5\u9c81\u73ed\u7ec4\u4ef6\u4e4b\u524d\u9700\u8981\u521d\u59cb\u5316\u4e00\u4e2a\u9ad8\u53ef\u7528\u7684kubernetes\u96c6\u7fa4\uff0c\u96c6\u7fa4\u6700\u4f4e\u914d\u7f6e\u5982\u4e0b\uff0c\u8ba1\u7b97\u8d44\u6e90\u4e0e\u5b58\u50a8\u8d44\u6e90\u6839\u636e\u76d1\u63a7\u6570\u636e\u91cf\u4ee5\u53ca\u5b58\u50a8\u6570\u636e\u91cf\u9002\u5f53\u52a0\u5927\u3002 \u89d2\u8272 \u670d\u52a1\u5668\u540d\u79f0 \u4e3b\u673a\u540d \u8ba1\u7b97\u8d44\u6e90 \u78c1\u76d8\uff08\u89c1\u5b58\u50a8\u8d44\u6e90\u89c4\u5212\uff09 \u63a8\u8350\u7f51\u5361 \u64cd\u4f5c\u7cfb\u7edf \u63cf\u8ff0 \u7f51\u7edc\u7c7b\u578b \u5907\u6ce8 \u7ba1\u7406\u8282\u70b9\uff083\u53f0\u505a\u9ad8\u53ef\u7528\uff0c\u865a\u62df\u673a\u6216\u8005\u7269\u7406\u673a\uff09 Master1 x.x.x.x 8C16G 1\u3001\u6bcf\u53f0\u865a\u673a\u7cfb\u7edf\u76d8\uff0c20G\uff0c \u5173\u95edswap 2\u3001\u6bcf\u53f0\u865a\u673a\u5355\u72ec\u52a0\u4e00\u5757\uff08\u88f8\uff09\u6570\u636e\u76d8\uff0c100-500G\uff0c\u4f5c\u4e3aDocker VG 3\u3001\u6bcf\u53f0\u865a\u673a\u5355\u72ec\u52a0\u4e00\u5757\u6570\u636e\u76d8\uff0c50G\uff0c\u4f5c\u4e3aETCD\u6570\u636e\u5b58\u50a8 \u6302\u8f7d\u70b9\uff1a/var/lib/etcd \u6587\u4ef6\u7cfb\u7edf\uff1a xfs \u6570\u91cf\uff1a1 Centos7.x \u7ba1\u7406\u8282\u70b9 \u7269\u7406\u7f51\u7edc\u6216VLAN\u7c7b\u578b\u7684\u865a\u62df\u7f51\u7edc \u7ba1\u7406\u8282\u70b9\uff083\u53f0\u505a\u9ad8\u53ef\u7528\uff0c\u865a\u62df\u673a\u6216\u8005\u7269\u7406\u673a\uff09 Master2 x.x.x.x 8C16G \u540c\u4e0a \u6570\u91cf\uff1a1 Centos7.x \u7ba1\u7406\u8282\u70b9 \u540c\u4e0a \u7ba1\u7406\u8282\u70b9\uff083\u53f0\u505a\u9ad8\u53ef\u7528\uff0c\u865a\u62df\u673a\u6216\u8005\u7269\u7406\u673a\uff09 Master3 x.x.x.x 8C16G \u540c\u4e0a \u6570\u91cf\uff1a1 Centos7.x \u7ba1\u7406\u8282\u70b9 \u540c\u4e0a \u8ba1\u7b97\u8282\u70b9 Worker1 x.x.x.x 8C32G 1\u3001\u6bcf\u53f0\u865a\u673a\u7cfb\u7edf\u76d8\uff0c20G\uff0c \u5173\u95edswap 2\u3001\u6bcf\u53f0\u865a\u673a\u5355\u72ec\u52a0\u4e00\u5757\uff08\u88f8\uff09\u6570\u636e\u76d8\uff0c100G-500G\uff0c\u4f5c\u4e3aDocker VG \u6570\u91cf\uff1a1 Centos7.x \u8ba1\u7b97\u8282\u70b9 \u540c\u4e0a \u8ba1\u7b97\u8282\u70b9 Worker2 x.x.x.x 8C32G \u540c\u4e0a \u6570\u91cf\uff1a1 Centos7.x \u8ba1\u7b97\u8282\u70b9 \u540c\u4e0a \u8ba1\u7b97\u8282\u70b9 Worker....... x.x.x.x 8C32G \u540c\u4e0a \u6570\u91cf\uff1a1 Centos7.x \u8ba1\u7b97\u8282\u70b9 \u540c\u4e0a \u955c\u50cf\u4ed3\u5e93\uff08\u865a\u62df\u673a\uff09 Registry1 x.x.x.x 4C8G \u540c\u4e0a \u6570\u91cf\uff1a1 Centos7.x \u4ed3\u5e93 \u540c\u4e0a \u955c\u50cf\u4ed3\u5e93\uff08\u865a\u62df\u673a\uff09 Registr2 x.x.x.x 4C8G \u540c\u4e0a \u6570\u91cf\uff1a1 Centos7.x \u4ed3\u5e93 \u540c\u4e0a MasterLb1 Master\u8282\u70b9\u8d1f\u8f7d\u5747\u8861 MasterLb1 x.x.x.x 4C8G \u540c\u4e0a \u6570\u91cf\uff1a1 Centos7.x \u8d1f\u8f7d\u5747\u8861 \u540c\u4e0a \u6216\u8005\u4f7f\u7528slb MasterLb2 Master\u8282\u70b9\u8d1f\u8f7d\u5747\u8861 MasterLb2 x.x.x.x 4C8G \u540c\u4e0a \u6570\u91cf\uff1a1 Centos7.x \u8d1f\u8f7d\u5747\u8861 \u540c\u4e0a \u6216\u8005\u4f7f\u7528slb Registry\u8282\u70b9\u8d1f\u8f7d\u5747\u8861 RouterLb1 x.x.x.x 4C8G \u540c\u4e0a \u6570\u91cf\uff1a1 Centos7.x \u8d1f\u8f7d\u5747\u8861 \u540c\u4e0a \u6216\u8005\u4f7f\u7528slb Registry\u8282\u70b9\u8d1f\u8f7d\u5747\u8861 RouterLb2 x.x.x.x 4C8G \u540c\u4e0a \u6570\u91cf\uff1a1 Centos7.x \u8d1f\u8f7d\u5747\u8861 \u540c\u4e0a \u6216\u8005\u4f7f\u7528slb \u5b89\u88c5Kubernetes \u73af\u5883\u4fe1\u606f \u4e3b\u673a\u540d IP\u5730\u5740 master0 192.168.0.2 master1 192.168.0.3 master2 192.168.0.4 node0 192.168.0.5 \u670d\u52a1\u5668\u5bc6\u7801\uff1a123456 \u9ad8\u53ef\u7528\u5b89\u88c5\u6559\u7a0b \u53ea\u9700\u8981\u51c6\u5907\u597d\u670d\u52a1\u5668\uff0c\u5728\u4efb\u610f\u4e00\u53f0\u670d\u52a1\u5668\u4e0a\u6267\u884c\u4e0b\u9762\u547d\u4ee4\u5373\u53ef # \u4e0b\u8f7d\u5e76\u5b89\u88c5sealos, sealos\u662f\u4e2agolang\u7684\u4e8c\u8fdb\u5236\u5de5\u5177\uff0c\u76f4\u63a5\u4e0b\u8f7d\u62f7\u8d1d\u5230bin\u76ee\u5f55\u5373\u53ef, release\u9875\u9762\u4e5f\u53ef\u4e0b\u8f7d wget -c https://sealyun.oss-cn-beijing.aliyuncs.com/latest/sealos && \\ chmod +x sealos && mv sealos /usr/bin # \u4e0b\u8f7d\u79bb\u7ebf\u8d44\u6e90\u5305 wget -c https://sealyun.oss-cn-beijing.aliyuncs.com/2fb10b1396f8c6674355fcc14a8cda7c-v1.20.0/kube1.20.0.tar.gz # \u5b89\u88c5\u4e00\u4e2a\u4e09master\u7684kubernetes\u96c6\u7fa4 $ sealos init --passwd '123456' \\ --master 192.168.0.2 --master 192.168.0.3 --master 192.168.0.4 \\ --node 192.168.0.5 \\ --pkg-url /root/kube1.20.0.tar.gz \\ --version v1.20.0 \u53c2\u6570\u542b\u4e49 \u53c2\u6570\u540d \u542b\u4e49 \u793a\u4f8b passwd \u670d\u52a1\u5668\u5bc6\u7801 123456 master k8s master\u8282\u70b9IP\u5730\u5740 192.168.0.2 node k8s node\u8282\u70b9IP\u5730\u5740 192.168.0.3 pkg-url \u79bb\u7ebf\u8d44\u6e90\u5305\u5730\u5740\uff0c\u652f\u6301\u4e0b\u8f7d\u5230\u672c\u5730\uff0c\u6216\u8005\u4e00\u4e2a\u8fdc\u7a0b\u5730\u5740 /root/kube1.20.0.tar.gz version \u8d44\u6e90\u5305 \u5bf9\u5e94\u7684\u7248\u672c v1.20.0 \u589e\u52a0master sealos join --master 192.168.0.6 --master 192.168.0.7 sealos join --master 192.168.0.6-192.168.0.9 # \u6216\u8005\u591a\u4e2a\u8fde\u7eedIP \u589e\u52a0node sealos join --node 192.168.0.6 --node 192.168.0.7 sealos join --node 192.168.0.6-192.168.0.9 # \u6216\u8005\u591a\u4e2a\u8fde\u7eedIP \u5220\u9664\u6307\u5b9amaster\u8282\u70b9 sealos clean --master 192.168.0.6 --master 192.168.0.7 sealos clean --master 192.168.0.6-192.168.0.9 # \u6216\u8005\u591a\u4e2a\u8fde\u7eedIP \u5220\u9664\u6307\u5b9anode\u8282\u70b9 sealos clean --node 192.168.0.6 --node 192.168.0.7 sealos clean --node 192.168.0.6-192.168.0.9 # \u6216\u8005\u591a\u4e2a\u8fde\u7eedIP \u6e05\u7406\u96c6\u7fa4 sealos clean --all \u5907\u4efd\u96c6\u7fa4 sealos etcd save \u5b89\u88c5\u670d\u52a1\u7f51\u683c \u5b89\u88c5Istio istio\u7528\u4e8e\u7ba1\u7406\u94f6\u6cb3\u7684\u670d\u52a1\uff0c\u670d\u52a1\u4e4b\u95f4\u7684\u6743\u9650\u8bbe\u7f6e\u3002\u4e0b\u8f7distioctl\u5230\u5b89\u88c5\u73af\u5883 \u6267\u884c\u5b89\u88c5\u547d\u4ee4 \u79bb\u7ebf\u955c\u50cf\u5217\u8868 docker.io/istio/pilot:1.9.1 docker.io/istio/proxyv2:1.9.1 \u540c\u6b65\u955c\u50cf for i in `cat image.txt`;do docker pull $i;done for i in `cat image.txt`;do docker tag $i harbor.inner.galaxy.ksyun.com/istio/${i##*/};done for i in `cat image.txt`;do docker push harbor.inner.galaxy.ksyun.com/istio/${i##*/};done \u5b89\u88c5 istioctl install --set profile=demo -y --set values.global.hub=\"harbor.inner.galaxy.ksyun.com/istio\" \u5b89\u88c5\u76d1\u63a7\u544a\u8b66\u7ec4\u4ef6 \u5b89\u88c5Prometheus+Alertmanager kubectl create ns monitoring rm -rf custom-values.yaml export REGISTRY=\"harbor.inner.galaxy.ksyun.com\" cat <<EOF > custom-values.yaml prometheusOperator: image: repository: $REGISTRY/luban/prometheus-operator tag: v0.45.0 prometheusConfigReloaderImage: repository: $REGISTRY/luban/prometheus-config-reloader tag: v0.45.0 admissionWebhooks: patch: image: repository: $REGISTRY/luban/kube-webhook-certgen tag: v1.5.0 alertmanager: alertmanagerSpec: image: repository: $REGISTRY/luban/alertmanager tag: v0.21.0 config: global: resolve_timeout: 5m route: group_by: ['alertname'] group_wait: 30s group_interval: 1m repeat_interval: 1m prometheus: prometheusSpec: image: repository: $REGISTRY/luban/prometheus tag: v2.24.0 kube-state-metrics: image: repository: $REGISTRY/luban/kube-state-metrics tag: v1.9.8 grafana: image: repository: $REGISTRY/luban/grafana tag: 7.4.2 sidecar: image: repository: $REGISTRY/luban/k8s-sidecar tag: 1.10.6 adminPassword: Kingsoft123 prometheus-node-exporter: image: repository: $REGISTRY/luban/node-exporter tag: v1.0.1 EOF helm install pm ./kube-prometheus-stack-13.13.1.tgz -n monitoring -f custom-values.yaml \u5b89\u88c5\u9c81\u73edAgent \u76ee\u524d\u9c81\u73ed\u7684\u7248\u672c\u53d1\u5e03\u653e\u5728\u9752\u5c9b5\u670d\u52a1\u5668\uff0c\u540e\u7eed\u4f1a\u8ddf\u94f6\u6cb3\u4ea7\u54c1\u4e00\u4e0b\u6253\u5305\u53d1\u5e03\u3002 centos65\u7248\u672c\u7269\u7406\u673a\u670d\u52a1\u5668 #!/bin/sh export agent_http_server=\"http://10.177.152.168:8888/luban/luban_on_k8s/agent/luban_agent\" export bin_dir=\"/opt/luban/bin\" mkdir -p $bin_dir ps -aux |grep luban_agent|grep -v grep|awk '{print $2}'|xargs kill -9 rm -rf $bin_dir/luban_agent curl -o $bin_dir/luban_agent $agent_http_server chmod a+x $bin_dir/luban_agent rm -rf /var/log/luban_agent.log cat > /opt/luban/bin/start_agent.sh << EOF nohup $bin_dir/luban_agent >/var/log/luban_agent.log 2>&1 & EOF chmod a+x /opt/luban/bin/start_agent.sh cat > /opt/luban/bin/stop_agent.sh << EOF ps -aux |grep luban_agent|grep -v grep|awk '{print $2}'|xargs kill -9 EOF chmod a+x /opt/luban/bin/stop_agent.sh cat > /etc/init.d/luban_agent << EOF #!/bin/bash # # /etc/rc.d/init.d/luban_agent # # Luban Agent # # description: Luban Agent # processname: luban_agent # chkconfig: - 85 15 # Source function library. . /etc/rc.d/init.d/functions PROGNAME=luban_agent PROG=/opt/luban/bin/\\$PROGNAME USER=root LOGFILE=/var/log/luban_agent.log LOCKFILE=/var/run/\\$PROGNAME.pid start() { echo -n \"Starting \\$PROGNAME: \" daemon --user $USER --pidfile=\"\\$LOCKFILE\" \"\\$PROG &>\\$LOGFILE &\" echo } stop() { echo -n \"Shutting down $PROGNAME: \" kill -9 \\`pidof \\$PROGNAME\\` rm -f $LOCKFILE echo } case \"\\$1\" in start) start ;; stop) stop ;; status) status $PROGNAME ;; restart) stop start ;; *) echo \"Usage: service luban_agent {start|stop|status|restart}\" exit 1 ;; esac EOF chmod a+x /etc/init.d/luban_agent service luban_agent start chkconfig luban_agent on centos73\u7684\u7269\u7406\u670d\u52a1\u5668 #!/bin/sh export http_server=\"http://10.177.9.11:8888/luban/\" export bin_dir=\"/opt/luban/bin\" mkdir -p $bin_dir rm -rf $bin_dir/* curl -o $bin_dir/luban_agent $http_server/luban_agent chmod a+x $bin_dir/* rm -rf /var/log/luban_agent.log cat > /opt/luban/bin/start_agent.sh << EOF nohup $bin_dir/luban_agent >/var/log/luban_agent.log 2>&1 & EOF chmod a+x /opt/luban/bin/start_agent.sh cat > /opt/luban/bin/stop_agent.sh << EOF ps -aux |grep luban_agent|grep -v grep|awk '{print $2}'|xargs kill -9 EOF chmod a+x /opt/luban/bin/stop_agent.sh cat > /usr/lib/systemd/system/luban_agent.service << EOF [Unit] Description=Luban agent After=network.target [Service] ExecStart=/opt/luban/bin/luban_agent > /var/log/luban_agent.log ExecStop=kill -9 `pidof luban_agent` Type=notify User=root Group=root [Install] WantedBy=multi-user.target EOF systemctl enable luban_agent systemctl start luban_agent \u5b89\u88c5Node_exporter Node_exporter\u8d1f\u8d23\u76d1\u63a7\u7269\u7406\u673a\u670d\u52a1\u5668\u4ee5\u53ca\u90e8\u7f72\u94f6\u6cb3\u670d\u52a1\u7684\u865a\u62df\u673a\u76d1\u63a7\u4fe1\u606f\uff0c\u76d1\u63a7\u4e3b\u8981\u5305\u62eccpu\uff0c\u5185\u5b58\uff0c\u78c1\u76d8\uff0c\u7f51\u7edc \u5b89\u88c5\u6b65\u9aa4 #!/bin/sh export node_exporter_url=\"http://10.177.152.168:8888/luban/luban_on_k8s/install/node_exporter/node_exporter\" mkdir -p /opt/luban/bin/ export node_exporter_binary=\"/opt/luban/bin/node_exporter\" rm -rf $node_exporter_binary curl -o $node_exporter_binary ${node_exporter_url} chmod a+x $node_exporter_binary ps -aux |grep $node_exporter_binary|awk '{print $2}'|xargs kill -9 nohup $node_exporter_binary --collector.luban >/var/log/node_exporter.log 2>&1 & \u5b89\u88c5Process_exporter Process_exporter\u8d1f\u8d23\u76d1\u63a7\u7269\u7406\u673a\u670d\u52a1\u5668\u4ee5\u53ca\u90e8\u7f72\u94f6\u6cb3\u670d\u52a1\u7684\u865a\u62df\u673a\u76d1\u63a7\u4fe1\u606f\uff0c\u76d1\u63a7\u4e3b\u8981\u5305\u62ec\u8fdb\u7a0b\u76d1\u63a7 \u5b89\u88c5\u6b65\u9aa4 #!/bin/sh export process_exporter_url=\"http://10.177.152.168:8888/luban/luban_on_k8s/install/process_exporter/process-exporter\" mkdir -p /opt/luban/bin/ mkdir -p /opt/luban/conf/ export process_exporter_binary=\"/opt/luban/bin/process-exporter\" rm -rf $process_exporter_binary curl -o $process_exporter_binary ${process_exporter_url} chmod a+x $process_exporter_binary ps -aux |grep $process_exporter_binary |grep -v grep|awk '{print $2}'|xargs kill -9 if [ ! -f \"/opt/luban/conf/process-name.yaml\" ];then echo \"/opt/luban/conf/process-name.yaml not exist\" else cp -rf /opt/luban/conf/process-name.yaml /opt/luban/conf/process-name.yaml.bak fi cat > /opt/luban/conf/process-name.yaml <<EOF process_names: - name: \"{ {.Comm}}\" cmdline: - '.+' EOF nohup $process_exporter_binary -config.path=/opt/luban/conf/process-name.yaml >/var/log/process_exporter.log 2>&1 & \u5b89\u88c5Blackbox_exporter Blackbox_exporter\u8d1f\u8d23\u76d1\u63a7Ping\uff0cDNS\uff0cTelnet\u7b49\u76d1\u63a7 \u5b89\u88c5\u6b65\u9aa4 #!/bin/sh kubectl apply -f blackbox_alert.yaml -f blackbox_exporter.yaml -f blackbox_monitor.yaml -n monitoring \u5b89\u88c5Cluster_exporter Cluster_exporter\u8d1f\u8d23\u76d1\u63a7Ebs,Ks3\u7b49\u5b58\u50a8\u603b\u91cf\u4fe1\u606f\u4ee5\u53ca\u4f7f\u7528\u91cf\u4fe1\u606f\u7b49\u76d1\u63a7 \u5b89\u88c5\u6b65\u9aa4 #!/bin/sh kubectl apply -f cluster_exporter.yaml -n monitoring ## \u5b89\u88c5agent,\u4ee5\u4e0b10.177.9.1 \u7269\u7406\u670d\u52a1\u5668\u4e3a\u4f8b\uff0cssh\u767b\u5f55\u670d\u52a1\u5668,10.177.9.11\u4e3a\u9c81\u73ed\u7684http\u6587\u4ef6\u670d\u52a1\u5668 export http_server=\"http://10.177.9.11:8888/luban/\" export bin_dir=\"/opt/luban/bin\" mkdir -p $bin_dir rm -rf $bin_dir/* curl -o $bin_dir/node_exporter http://10.177.9.11:8888/luban/node_exporter curl -o $bin_dir/luban_agent http://10.177.9.11:8888/luban/luban_agent chmod a+x $bin_dir/* ## restart node_exporter ps -aux |grep node_exporter|grep -v grep|awk '{print $2}'|xargs kill -9 echo \"node_exporter shutdown [ok]\" ## restart luban_agent ps -aux |grep luban_agent|grep -v grep|awk '{print $2}'|xargs kill -9 echo \"luban_agent shutdown [ok]\" ## start node_exporter and luban_agent nohup $bin_dir/node_exporter --collector.luban >/var/log/node_exporter.log 2>&1 & echo \"node_exporter start [ok]\" nohup $bin_dir/luban_agent >/var/log/luban_agent.log 2>&1 & echo \"luban_agent start [ok]\" \u5b89\u88c5\u5b8c\u6210\u540e\u68c0\u67e5 node_exporter \u4ee5\u53caluban_agent \u662f\u5426\u6b63\u786e\u8fd0\u884c ps aux |grrep node_exporter ps aux |grrep luban_agent \u5b89\u88c5\u81ea\u5b9a\u4e49\u76d1\u63a7\u811a\u672c \u6b64\u6b65\u9aa4\u7531\u8fd0\u7ef4\u540c\u5b66\u5b89\u88c5\u5e95\u5c42\u94f6\u6cb3\u73af\u5883\u65f6\u5019\uff0c\u81ea\u52a8\u5316\u5b89\u88c5\uff0c\u81ea\u5b9a\u4e49\u76d1\u63a7\u811a\u672c\u5b58\u653e\u4f4d\u7f6e http://newgit.op.ksyun.com/galaxy_cloud/monitor_scripts/tree/master/sdn_monitor_shell\uff0c\u7531\u8fd0\u7ef4\u540c\u5b66\u81ea\u52a8\u5316\u5b89\u88c5\uff0c\u4ee5\u4e0b\u793a\u4f8b\u4f9b\u7814\u53d1\u540c\u5b66\u53c2\u8003 \u4ee5\u4e0bmem_monitor.sh\u4e3a\u4f8b\uff0c\u811a\u672c\u5185\u5bb9\u5982\u4e0b\uff1a #!/bin/bash # \u5185\u5b58\u76d1\u63a7\u811a\u672c source /etc/profile function send_data(){ ts=`date +%s`; metric=$1; endpoint=$4; value=$2; tags=$3 curl -X POST -d \"[{\\\"metric\\\": \\\"$metric\\\", \\\"endpoint\\\": \\\"$endpoint\\\", \\\"timestamp\\\": $ts,\\\"step\\\": 60,\\\"value\\\": $value,\\\"counterType\\\": \\\"GAUGE\\\",\\\"tags\\\": \\\"$tags\\\"}]\" http://127.0.0.1:1988/v1/push } item='mem_check' item1='crash_check' day=`date +%d` tag1='metric=crash_status' tag2='metric=/var/log/mcelog' tag3='metric=/var/log/messages' tag4='metric=/var/log/dmesg' now_time=`date +%F` hostn=`hostname` # \u5224\u65ad\u662f\u5426\u6709crash\u65e5\u5fd7 function crashlog(){ crash_log=`ls /var/crash/|grep \"$now_time\" |wc -l` if [ $crash_log -eq 0 ];then crash_status=1 else crash_status=0 fi echo $item $crash_status $tag1 $hostn crash send_data $item1 $crash_status $tag1 $hostn crash } # \u5224\u65ad\u662f\u5426\u6709\u5185\u5b58\u65e5\u5fd7\u6587\u4ef6 function mcelog(){ test -a /var/log/mcelog if [ $? -eq 0 ];then counts=`cat /var/log/mcelog|wc -l` if [ $counts -eq 0 ];then mcelog_status=1 else mcelog_status=0 fi else mcelog_status=1 fi echo $item $mcelog_status $tag2 $hostn mcelog send_data $item $mcelog_status $tag2 $hostn mcelog } # \u5224\u65ad\u65e5\u5fd7 function messagelog (){ message_log=`tail -n500 /var/log/messages|egrep -i 'err|fail' |egrep -i \"memory\" |egrep -v 'nova.compute.manager'|wc -l` if [ $message_log -gt 1 ];then mem_status=0 else mem_status=1 fi echo $item $mem_status $tag3 $hostn message send_data $item $mem_status $tag3 $hostn message } # \u5224\u65ad\u65e5\u5fd7 function dmesglog (){ dme_log1=`tail -n500 /var/log/dmesg|egrep -i \"err|fail\" |egrep -i \"memory\" |wc -l` #dme_log2=`ssh $i dmesg |tail -n200 |egrep -i 'error|fail'|wc -l` if [ $dme_log1 -gt 1 ];then dme_status=0 else dme_status=1 fi echo $item $dme_status $tag4 $hostn dmesg send_data $item $dme_status $tag4 $hostn dmesg } crashlog mcelog messagelog dmesglog \u914d\u7f6e\u7269\u7406\u670d\u52a1\u5668\u4e0a\u7684crontab\uff0c\u5982\u4e0b\u56fe\u6240\u793a cat /etc/crontab SHELL=/bin/bash PATH=/sbin:/bin:/usr/sbin:/usr/bin MAILTO=root HOME=/ # For details see man 4 crontabs # Example of job definition: # .---------------- minute (0 - 59) # | .------------- hour (0 - 23) # | | .---------- day of month (1 - 31) # | | | .------- month (1 - 12) OR jan,feb,mar,apr ... # | | | | .---- day of week (0 - 6) (Sunday=0 or 7) OR sun,mon,tue,wed,thu,fri,sat # | | | | | # * * * * * user-name command to be executed */1 * * * * root bash /opt/luban/script/mem_monitor.sh \u5b89\u88c5\u65e5\u5fd7\u7ec4\u4ef6 \u76ee\u524d\u65e5\u5fd7\u7684\u7ec4\u4ef6\u91c7\u7528filebeat+kafka+logstash+ELK\u67b6\u6784,\u53c2\u7167http://ezone.ksyun.com/ezCode/luban/luban_on_k8s/tree\u4e2d\u7684logging\u6587\u4ef6\u5939\u4e2d\u7684README.md \u9700\u8981\u79bb\u7ebf\u955c\u50cf\u5217\u8868 harbor.inner.galaxy.ksyun.com/luban/elasticsearch:7.10.1 harbor.inner.galaxy.ksyun.com/luban/kibana:7.10.1 harbor.inner.galaxy.ksyun.com/luban/filebeat:7.10.1 harbor.inner.galaxy.ksyun.com/luban/kafka:2.7.0-debian-10-r1 harbor.inner.galaxy.ksyun.com/luban/zookeeper:3.6.2-debian-10-r89 harbor.inner.galaxy.ksyun.com/luban/logstash:7.10.1 \u5b89\u88c5Elasticsearch kubectl create ns elastic-system export REGISTRY=\"harbor.inner.galaxy.ksyun.com\" helm install es ./elasticsearch-7.10.1.tgz -n elastic-system --set imageTag=7.10.1 --set persistence.enabled=false --set image=$REGISTRY/luban/elasticsearch \u5b89\u88c5Kibana kubectl create ns elastic-system export REGISTRY=\"harbor.inner.galaxy.ksyun.com\" helm install kibana -n elastic-system ./kibana-7.10.1.tgz --set imageTag=7.10.1 --set image=$REGISTRY/luban/kibana \u5b89\u88c5Kafka \u6ce8\u610fkafka\u96c6\u7fa4\u9700\u8981\u96c6\u7fa4\u5916\u8bbf\u95ee\u6743\u9650\uff0c\u6700\u597d\u589e\u52a0eip\u7684slb\u7ed1\u5b9akubernetes\u96c6\u7fa4\u7684\u4e09\u4e2amaster\u8282\u70b9 kubectl create ns elastic-system export REGISTRY=\"harbor.inner.galaxy.ksyun.com\" helm install kafka ./kafka-12.4.2.tgz -n elastic-system --set persistence.enabled=false --set image.registry=$REGISTRY --set image.repository=luban/kafka --set image.tag=2.7.0-debian-10-r1 --set zookeeper.image.registry=$REGISTRY --set zookeeper.image.repository=luban/zookeeper --set zookeeper.image.tag=3.6.2-debian-10-r89 --set zookeeper.persistence.enabled=false --set replicaCount=3 \\ --set externalAccess.enabled=true \\ --set externalAccess.service.type=NodePort \\ --set externalAccess.autoDiscovery.enabled=false \\ --set serviceAccount.create=true \\ --set rbac.create=true \\ --set externalAccess.service.nodePorts='{30092,30093,30094}' \\ --set externalAccess.service.domain=10.177.152.168 ##3*master\u8282\u70b9\u7684\u7684slb\u7684eip \u5b89\u88c5Filebeat\uff08\u96c6\u7fa4\u5185\u90e8\uff0c\u7528\u4e8e\u6536\u96c6\u5bb9\u5668\u65e5\u5fd7\uff0c\u76f4\u63a5\u4e0a\u4f20es\u96c6\u7fa4\uff09 kubectl create ns elastic-system export REGISTRY=\"harbor.inner.galaxy.ksyun.com\" helm install filebeat ./filebeat-7.10.1.tgz -n elastic-system --set imageTag=7.10.1 --set image=$REGISTRY/luban/filebeat \u5b89\u88c5Filebeat\uff08\u96c6\u7fa4\u5916\u90e8\uff0c\u7528\u4e8e\u6536\u96c6\u670d\u52a1\u5668\u6216\u8005\u5176\u4ed6\u7269\u7406\u8bbe\u5907\u65e5\u5fd7\uff0c\u5199\u5165kafka\uff09 rpm -ivh filebeat-7.10.1-x86_64.rpm \u4fee\u6539/etc/filebeat/filebeat.yml\u914d\u7f6e\uff0c\u5b98\u65b9\u5730\u5740\uff1ahttps://www.elastic.co/guide/en/beats/filebeat/current/configuring-howto-filebeat.html \u5b89\u88c5Logstash \u6ce8\u610f\u4fee\u6539logstash\u7684\u914d\u7f6e\u6587\u4ef6values.yaml kubectl create ns elastic-system export REGISTRY=\"harbor.inner.galaxy.ksyun.com\" helm install logstash ./logstash -n elastic-system --set imageTag=7.10.1 --set image=$REGISTRY/luban/logstash \u5b89\u88c5\u81ea\u52a8\u6e05\u7406Curator helm install curator ./elasticsearch-curator -n elastic-system --set image.repository=harbor.inner.galaxy.ksyun.com/luban/curator --set image.tag=5.7.6 \u4ee5\u4e0b\u662f\u81ea\u52a8\u5316\u811a\u672c kubectl create ns elastic-system export REGISTRY=\"harbor.inner.galaxy.ksyun.com\" helm install es ./elasticsearch-7.10.1.tgz -n elastic-system --set imageTag=7.10.1 --set persistence.enabled=false --set image=$REGISTRY/luban/elasticsearch helm install kibana -n elastic-system ./kibana-7.10.1.tgz --set imageTag=7.10.1 --set image=$REGISTRY/luban/kibana helm install filebeat ./filebeat-7.10.1.tgz -n elastic-system --set imageTag=7.10.1 --set image=$REGISTRY/luban/filebeat ## \u6ce8\u610f\u4fee\u6539kafka\u7684externalAccess.service.domain\uff0c\u7528\u4e8e\u5176\u4ed6\u7269\u7406\u670d\u52a1\u5668filebeat\u914d\u7f6e\uff0c10.177.152.168\u5e94\u4e3aK8s\u7684Master\u8282\u70b9\u8d1f\u8f7d\u5747\u8861IP helm install kafka ./kafka-12.4.2.tgz -n elastic-system --set persistence.enabled=false --set image.registry=$REGISTRY --set image.repository=luban/kafka --set image.tag=2.7.0-debian-10-r1 --set zookeeper.image.registry=$REGISTRY --set zookeeper.image.repository=luban/zookeeper --set zookeeper.image.tag=3.6.2-debian-10-r89 --set zookeeper.persistence.enabled=false --set replicaCount=3 \\ --set externalAccess.enabled=true \\ --set externalAccess.service.type=NodePort \\ --set externalAccess.autoDiscovery.enabled=false \\ --set serviceAccount.create=true \\ --set rbac.create=true \\ --set externalAccess.service.nodePorts='{30092,30093,30094}' \\ --set externalAccess.service.domain=10.177.152.168 helm install logstash ./logstash -n elastic-system --set imageTag=7.10.1 --set image=$REGISTRY/luban/logstash helm install curator ./elasticsearch-curator -n elastic-system --set image.repository=$REGISTRY/luban/curator --set image.tag=5.7.6 \u5b89\u88c5Cmdb\u7ec4\u4ef6 \u53c2\u7167aws\u7684dgraph HA\u67b6\u6784\uff0c\u539f\u6587\u5730\u5740\uff1a https://aws.amazon.com/cn/blogs/opensource/dgraph-on-aws-setting-up-a-horizontally-scalable-graph-database/ MANIFEST=\"https://raw.githubusercontent.com/dgraph-io/dgraph/master/contrib/config/kubernetes/dgraph-ha/dgraph-ha.yaml\" kubectl apply --filename $MANIFEST \u5b89\u88c5\u8ba4\u8bc1\u670d\u52a1 \u5b89\u88c5Dex \u5f00\u6e90\u9879\u76eedex\uff0c\u4e00\u4e2a\u57fa\u4e8eOpenID Connect\u7684\u8eab\u4efd\u670d\u52a1\u7ec4\u4ef6\u3002 CoreOS\u5df2\u7ecf\u5c06\u5b83\u7528\u4e8e\u751f\u4ea7\u73af\u5883\uff0c\u7528\u6237\u8ba4\u8bc1\u548c\u6388\u6743\u662f\u5e94\u7528\u5b89\u5168\u7684\u4e00\u4e2a\u91cd\u8981\u90e8\u5206\uff0c\u7528\u6237\u8eab\u4efd\u7ba1\u7406\u672c\u8eab\u4e5f\u662f\u4e00\u4e2a\u7279\u522b\u4e13\u4e1a\u548c\u590d\u6742\u7684\u95ee\u9898\uff0c\u5c24\u5176\u5bf9\u4e8e\u4f01\u4e1a\u5e94\u7528\u800c\u8a00\uff0c \u5b89\u5168\u7684\u8fdb\u884c\u8ba4\u8bc1\u548c\u6388\u6743\u662f\u5fc5\u9009\u9879\uff0cdex\u65e0\u7591\u662f\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u7684\u4e00\u5927\u5229\u5668\u3002 ```shell kubectl create ns dex kubectl -n dex delete secret dex.example.com.tls kubectl -n dex create secret tls dex.example.com.tls --cert=./ssl/cert.pem --key=./ssl/key.pem cat <<EOF | kubectl -n dex apply -f - apiVersion: apps/v1 kind: Deployment metadata: labels: app: dex name: dex namespace: dex spec: replicas: 1 selector: matchLabels: app: dex template: metadata: labels: app: dex spec: serviceAccountName: dex # This is created below containers: - image: harbor.inner.galaxy.ksyun.com/luban/dex:v2.27.0 #or quay.io/dexidp/dex:v2.26.0 name: dex command: [\"/usr/local/bin/dex\", \"serve\", \"/etc/dex/cfg/config.yaml\"] ports: - name: https containerPort: 5556 volumeMounts: - name: config mountPath: /etc/dex/cfg - name: tls mountPath: /etc/dex/tls - name: data mountPath: /data volumes: - name: config configMap: name: dex items: - key: config.yaml path: config.yaml - name: tls secret: secretName: dex.example.com.tls - name: data emptyDir: {} apiVersion: v1 kind: ConfigMap metadata: name: dex namespace: dex data: config.yaml: | issuer: https://luban.dex.galaxy.cloud/dex storage: type: memory logger: level: debug format: text web: http: 0.0.0.0:5556 expiry: signingKeys: \"6h\" idTokens: \"24h\" connectors: - type: ldap name: ActiveDirectory id: ladp config: host: openldap:389 insecureNoSSL: true insecureSkipVerify: true bindDN: cn=admin,dc=galaxy,dc=cloud bindPW: Kingsoft123 usernamePrompt: Email Address userSearch: baseDN: ou=People,dc=galaxy,dc=cloud filter: \"(objectClass=inetOrgPerson)\" username: mail idAttr: uid emailAttr: mail nameAttr: uid groupSearch: baseDN: ou=Groups,dc=galaxy,dc=cloud filter: \"(objectClass=groupOfUniqueNames)\" userAttr: uid groupAttr: memberUid nameAttr: cn oauth2: skipApprovalScreen: true logger: level: \"debug\" format: text staticClients: - id: oidc-auth-client redirectURIs: - 'https://luban.auth.galaxy.cloud/callback' - 'https://luban.idp.galaxy.cloud/idp/v1/token/callback' - 'http://localhost/idp/v1/token/callback' name: 'oidc-auth-client' secret: ZXhhbXBsZS1hcHAtc2VjcmV0 apiVersion: v1 kind: Service metadata: name: dex namespace: dex spec: ports: - name: dex port: 5556 protocol: TCP targetPort: 5556 selector: app: dex apiVersion: v1 kind: ServiceAccount metadata: labels: app: dex name: dex namespace: dex apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: dex rules: - apiGroups: [\"dex.coreos.com\"] # API group created by dex resources: [\" \"] verbs: [\" \"] - apiGroups: [\"apiextensions.k8s.io\"] resources: [\"customresourcedefinitions\"] verbs: [\"create\"] # To manage its own resources, dex must be able to create customresourcedefinitions apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: dex roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: dex subjects: - kind: ServiceAccount name: dex # Service account assigned to the dex pod, created above namespace: dex # The namespace dex is running in apiVersion: apiextensions.k8s.io/v1beta1 kind: CustomResourceDefinition metadata: name: authcodes.dex.coreos.com spec: group: dex.coreos.com names: kind: AuthCode listKind: AuthCodeList plural: authcodes singular: authcode scope: Namespaced version: v1 apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRole metadata: name: dex rules: - apiGroups: [\"dex.coreos.com\"] # API group created by dex resources: [\" \"] verbs: [\" \"] - apiGroups: [\"apiextensions.k8s.io\"] resources: [\"customresourcedefinitions\"] verbs: [\"create\"] # To manage its own resources identity must be able to create customresourcedefinitions. apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: dex roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: dex subjects: - kind: ServiceAccount name: dex # Service account assigned to the dex pod. namespace: dex EOF \u200b \u5b89\u88c5Ldap Open-ldap kubectl create ns dex helm install openldap ./openldap-1.2.7.tgz -f online-values.yaml -n dex ## get password $ kubectl -n dex get secret openldap -o jsonpath=\"{.data.LDAP_ADMIN_PASSWORD}\" | base64 --decode; echo $ kubectl -n dex get secret openldap -o jsonpath=\"{.data.LDAP_CONFIG_PASSWORD}\" | base64 --decode; echo # node add ldap label $ kubectl label nodes ip-10-178-224-65 luban/ldap=\"\" --overwrite ldapsearch -x -H ldap://openldap:389 \\ -b dc=galaxy,dc=cloud \\ -D \"cn=admin,dc=galaxy,dc=cloud\" \\ -w Kingsoft123 \u914d\u7f6eKubernetes\u8ba4\u8bc1\u670d\u52a1 ## set kubernetes apiserver - --oidc-issuer-url=https://luban.dex.galaxy.cloud/dex - --oidc-client-id=oidc-auth-client - --oidc-ca-file=/etc/kubernetes/ssl/dex-ca.pem - --oidc-username-claim=email - --oidc-groups-claim=groups \u5b89\u88c5\u9c81\u73ed\u670d\u52a1 \u5b89\u88c5\u9c81\u73edServer --- apiVersion: apps/v1 kind: Deployment metadata: labels: app: server name: server namespace: luban spec: progressDeadlineSeconds: 600 replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: app: server strategy: rollingUpdate: maxSurge: 25% maxUnavailable: 25% type: RollingUpdate template: metadata: creationTimestamp: null labels: app: server spec: containers: - image: harbor.inner.galaxy.ksyun.com/luban/server imagePullPolicy: Always name: server resources: {} terminationMessagePath: /dev/termination-log terminationMessagePolicy: File dnsPolicy: ClusterFirst restartPolicy: Always schedulerName: default-scheduler securityContext: {} terminationGracePeriodSeconds: 30 --- apiVersion: v1 kind: Service metadata: labels: app: server name: server namespace: luban spec: ports: - port: 80 protocol: TCP targetPort: 80 selector: app: server sessionAffinity: None type: ClusterIP --- apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: server-gateway spec: selector: istio: ingressgateway # use Istio default gateway implementation servers: - port: number: 80 name: http protocol: HTTP hosts: - \"luban.server.galaxy.cloud\" --- apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: server spec: hosts: - \"luban.server.galaxy.cloud\" gateways: - server-gateway http: - match: - uri: prefix: / route: - destination: port: number: 80 host: server corsPolicy: allowOrigins: - exact: \"*\" allowMethods: - GET - POST - PATCH - PUT - DELETE - OPTIONS allowCredentials: false allowHeaders: - authorization maxAge: \"24h\" \u5b89\u88c5\u9c81\u73edIdp --- apiVersion: apps/v1 kind: Deployment metadata: labels: app: idp name: idp namespace: luban spec: progressDeadlineSeconds: 600 replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: app: idp strategy: rollingUpdate: maxSurge: 25% maxUnavailable: 25% type: RollingUpdate template: metadata: creationTimestamp: null labels: app: idp spec: containers: - args: - --tls_cert= - --tls_key= - --domain=luban.idp.galaxy.cloud - --ldap_base=dc=galaxy,dc=cloud - --ldap_bind_password=Kingsoft123 - --ldap_bind_user=admin - --ldap_group_ou=Groups - --ldap_host=ldap-openldap.dex - --ldap_port=389 - --ldap_user_ou=People - --listin_addr=0.0.0.0 - --listin_port=80 - --oauth_client_id=oidc-auth-client - --oauth_client_secrect=ZXhhbXBsZS1hcHAtc2VjcmV0 - --oauth_redirect_url=http://localhost/idp/v1/token/callback - --oauth_scopes=openid,profile,email,offline_access,groups - --oauth_token_url=http://dex.dex:5556/dex/token - --oauth_url=http://dex.dex:5556/dex/auth command: - /app image: harbor.inner.galaxy.ksyun.com/luban/idp imagePullPolicy: Always name: server resources: {} terminationMessagePath: /dev/termination-log terminationMessagePolicy: File dnsPolicy: ClusterFirst restartPolicy: Always schedulerName: default-scheduler securityContext: {} terminationGracePeriodSeconds: 30 --- apiVersion: v1 kind: Service metadata: labels: app: idp name: idp namespace: luban spec: ports: - port: 80 protocol: TCP targetPort: 80 selector: app: idp sessionAffinity: None type: ClusterIP --- apiVersion: networking.istio.io/v1beta1 kind: VirtualService metadata: name: idp namespace: luban spec: gateways: - idp-gateway hosts: - luban.idp.galaxy.cloud http: - corsPolicy: allowCredentials: false allowHeaders: - authorization allowMethods: - GET - POST - PATCH - PUT - DELETE - OPTIONS allowOrigins: - exact: '*' maxAge: 24h route: - destination: host: idp --- apiVersion: networking.istio.io/v1beta1 kind: Gateway metadata: name: idp-gateway namespace: luban spec: selector: istio: ingressgateway servers: - hosts: - luban.idp.galaxy.cloud port: name: https number: 443 protocol: HTTPS tls: mode: PASSTHROUGH - hosts: - luban.idp.galaxy.cloud port: name: http number: 80 protocol: HTTP \u5b89\u88c5\u9c81\u73edCmdbApi --- apiVersion: apps/v1 kind: Deployment metadata: labels: app: cmdb name: cmdb namespace: luban spec: progressDeadlineSeconds: 600 replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: app: cmdb strategy: rollingUpdate: maxSurge: 25% maxUnavailable: 25% type: RollingUpdate template: metadata: creationTimestamp: null labels: app: cmdb spec: containers: - env: - name: OS_USERNAME value: admin - name: OS_TENANT_NAME value: admin - name: OS_PASSWORD value: ksc - name: OS_AUTH_URL value: http://10.177.147.1:35357/v2.0 - name: OS_REGION_NAME value: SHPBSRegionOne image: harbor.inner.galaxy.ksyun.com/luban/cmdb-api imagePullPolicy: Always name: cmdb resources: {} terminationMessagePath: /dev/termination-log terminationMessagePolicy: File dnsPolicy: ClusterFirst restartPolicy: Always schedulerName: default-scheduler securityContext: {} terminationGracePeriodSeconds: 30 --- apiVersion: v1 kind: Service metadata: labels: app: cmdb name: cmdb namespace: luban spec: ports: - port: 80 protocol: TCP targetPort: 80 selector: app: cmdb sessionAffinity: None type: ClusterIP --- apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: cmdb-gateway spec: selector: istio: ingressgateway # use Istio default gateway implementation servers: - port: number: 80 name: http protocol: HTTP hosts: - \"luban.cmdb.galaxy.cloud\" --- apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: cmdb spec: hosts: - \"luban.cmdb.galaxy.cloud\" gateways: - cmdb-gateway http: - match: - uri: prefix: / route: - destination: port: number: 80 host: cmdb corsPolicy: allowOrigins: - exact: \"*\" allowMethods: - GET - POST - PATCH - PUT - DELETE - OPTIONS allowCredentials: false allowHeaders: - authorization maxAge: \"24h\" \u5b89\u88c5\u9c81\u73edSwagger --- apiVersion: apps/v1 kind: Deployment metadata: labels: app: swagger-ui name: swagger-ui namespace: luban spec: progressDeadlineSeconds: 600 replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: app: swagger-ui strategy: rollingUpdate: maxSurge: 25% maxUnavailable: 25% type: RollingUpdate template: metadata: creationTimestamp: null labels: app: swagger-ui spec: containers: - image: harbor.inner.galaxy.ksyun.com/luban/swagger-ui imagePullPolicy: Always name: swagger-ui resources: {} terminationMessagePath: /dev/termination-log terminationMessagePolicy: File dnsPolicy: ClusterFirst restartPolicy: Always schedulerName: default-scheduler securityContext: {} terminationGracePeriodSeconds: 30 --- apiVersion: v1 kind: Service metadata: labels: app: swagger-ui name: swagger-ui namespace: luban spec: ports: - port: 8080 protocol: TCP targetPort: 8080 selector: app: swagger-ui sessionAffinity: None type: ClusterIP --- apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: swagger-gateway spec: selector: istio: ingressgateway # use Istio default gateway implementation servers: - port: number: 80 name: http protocol: HTTP hosts: - \"luban.swagger.galaxy.cloud\" --- apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: swagger spec: hosts: - \"luban.swagger.galaxy.cloud\" gateways: - swagger-gateway http: - match: - uri: prefix: / route: - destination: port: number: 8080 host: swagger-ui #!/bin/sh kubectl apply -f cmdb-api.yaml -f luban-idp.yaml -f luban-server.yaml -f swagger.yaml -n luban \u5b89\u88c5\u544a\u8b66\u5bf9\u63a5ElasticSearch apiVersion: apps/v1 kind: Deployment metadata: labels: app: alertmanager-warning name: alertmanager-warning namespace: monitoring spec: progressDeadlineSeconds: 600 replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: app: alertmanager-warning strategy: rollingUpdate: maxSurge: 25% maxUnavailable: 25% type: RollingUpdate template: metadata: creationTimestamp: null labels: app: alertmanager-warning spec: containers: - image: harbor.inner.galaxy.ksyun.com/luban/alertmanager-output imagePullPolicy: Always name: alertmanager-output resources: {} terminationMessagePath: /dev/termination-log terminationMessagePolicy: File dnsPolicy: ClusterFirst restartPolicy: Always schedulerName: default-scheduler securityContext: {} terminationGracePeriodSeconds: 30 --- apiVersion: v1 kind: Service metadata: labels: app: alertmanager-warning name: alertmanager-warning namespace: monitoring spec: ports: - port: 8080 protocol: TCP targetPort: 8080 selector: app: alertmanager-warning sessionAffinity: None type: ClusterIP --- apiVersion: monitoring.coreos.com/v1alpha1 kind: AlertmanagerConfig metadata: labels: alertmanagerConfig: warning release: pm name: warning namespace: monitoring spec: receivers: - name: warning-hook webhookConfigs: - url: http://alertmanager-warning:8080/webhook route: groupBy: - alertname groupInterval: 1m groupWait: 30s receiver: warning-hook repeatInterval: 1m \u5b89\u88c5\u544a\u8b66\u5bf9\u63a5\u98de\u4e66 apiVersion: apps/v1 kind: Deployment metadata: labels: app: alertmanager-feishu name: alertmanager-feishu namespace: monitoring spec: progressDeadlineSeconds: 600 replicas: 0 revisionHistoryLimit: 10 selector: matchLabels: app: alertmanager-feishu strategy: rollingUpdate: maxSurge: 25% maxUnavailable: 25% type: RollingUpdate template: metadata: creationTimestamp: null labels: app: alertmanager-feishu spec: containers: - image: harbor.inner.galaxy.ksyun.com/luban/feishu-webhook:latest imagePullPolicy: Always name: feishu-webhook resources: {} terminationMessagePath: /dev/termination-log terminationMessagePolicy: File dnsPolicy: ClusterFirst restartPolicy: Always schedulerName: default-scheduler securityContext: {} terminationGracePeriodSeconds: 30 --- apiVersion: v1 kind: Service metadata: labels: app: alertmanager-feishu name: alertmanager-feishu namespace: monitoring spec: ports: - port: 8080 protocol: TCP targetPort: 8080 selector: app: alertmanager-feishu sessionAffinity: None type: ClusterIP --- apiVersion: monitoring.coreos.com/v1alpha1 kind: AlertmanagerConfig metadata: labels: alertmanagerConfig: feshu release: pm name: feishu namespace: monitoring spec: receivers: - name: feishu-hook webhookConfigs: - url: http://alertmanager-feishu:8080/alertmanager-alert route: groupBy: - alertname groupInterval: 1m groupWait: 30s receiver: feishu-hook repeatInterval: 1m \u5bfc\u5165\u94f6\u6cb3\u544a\u8b66\u89c4\u5219 kubectl apply -f prometheus-rules -n monitoring \u5b89\u88c5\u76d1\u63a7\u7269\u7406\u673a \u76ee\u524d\u9636\u6bb5\u9700\u8981\u624b\u52a8\u5b89\u88c5prometheus\u76d1\u63a7\u7684\u7269\u7406\u673a\u5217\u8868\uff0c\u540e\u7eed\u4f1a\u81ea\u52a8\u53d1\u73b0\u7269\u7406\u673a\u4ee5\u53ca\u90e8\u7f72\u94f6\u6cb3\u670d\u52a1\u7684\u865a\u62df\u673a\u5217\u8868\uff08\u6682\u65f6\u624b\u52a8\u5b89\u88c5\uff09 apiVersion: v1 kind: Service metadata: labels: k8s-app: external-nodes name: external-nodes namespace: monitoring spec: ports: - name: metrics port: 9100 protocol: TCP targetPort: 9100 sessionAffinity: None type: ClusterIP --- apiVersion: v1 kind: Endpoints metadata: labels: k8s-app: external-nodes name: external-nodes namespace: monitoring subsets: - addresses: - ip: 10.177.16.2 - ip: 10.177.16.3 - ip: 10.177.16.4 - ip: 10.177.16.5 - ip: 10.177.16.6 - ip: 10.177.16.7 - ip: 10.177.9.11 - ip: 10.178.225.17 ports: - name: metrics port: 9100 protocol: TCP --- apiVersion: monitoring.coreos.com/v1 kind: ServiceMonitor metadata: labels: k8s-app: external-nodes release: pm name: external-nodes namespace: monitoring spec: endpoints: - interval: 60s port: metrics namespaceSelector: matchNames: - monitoring selector: matchLabels: k8s-app: external-nodes \u5b89\u88c5\u9c81\u73ed\u63a7\u5236\u53f0 \u5b89\u88c5\u9c81\u73edBase --- apiVersion: apps/v1 kind: Deployment metadata: labels: app: console-base name: console-base namespace: luban-fe spec: progressDeadlineSeconds: 600 replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: app: console-base strategy: rollingUpdate: maxSurge: 25% maxUnavailable: 25% type: RollingUpdate template: metadata: creationTimestamp: null labels: app: console-base spec: containers: - image: harbor.inner.galaxy.ksyun.com/watt/luban-fe/console-base:latest imagePullPolicy: Always name: console-base resources: {} terminationMessagePath: /dev/termination-log terminationMessagePolicy: File dnsPolicy: ClusterFirst restartPolicy: Always schedulerName: default-scheduler securityContext: {} terminationGracePeriodSeconds: 30 --- apiVersion: v1 kind: Service metadata: labels: app: console-base name: console-base namespace: luban-fe spec: ports: - port: 80 protocol: TCP targetPort: 80 selector: app: console-base sessionAffinity: None --- apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: console-gateway spec: selector: istio: ingressgateway # use Istio default gateway implementation servers: - port: number: 80 name: http protocol: HTTP hosts: - \"luban.console.galaxy.cloud\" --- apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: console spec: hosts: - \"luban.console.galaxy.cloud\" gateways: - console-gateway http: - match: - uri: prefix: / route: - destination: port: number: 80 host: console-base corsPolicy: allowOrigins: - exact: \"*\" allowMethods: - GET - POST - PATCH - PUT - DELETE - OPTIONS allowCredentials: false allowHeaders: - authorization maxAge: \"24h\" \u5b89\u88c5\u9c81\u73edSystem apiVersion: apps/v1 kind: Deployment metadata: labels: app: console-system name: console-system namespace: luban-fe spec: progressDeadlineSeconds: 600 replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: app: console-system strategy: rollingUpdate: maxSurge: 25% maxUnavailable: 25% type: RollingUpdate template: metadata: creationTimestamp: null labels: app: console-system spec: containers: - image: harbor.inner.galaxy.ksyun.com/watt/luban-fe/console-system:latest imagePullPolicy: Always name: console-system resources: {} terminationMessagePath: /dev/termination-log terminationMessagePolicy: File dnsPolicy: ClusterFirst restartPolicy: Always schedulerName: default-scheduler securityContext: {} terminationGracePeriodSeconds: 30 --- apiVersion: v1 kind: Service metadata: labels: app: console-system name: console-system namespace: luban-fe spec: ports: - port: 80 protocol: TCP targetPort: 80 selector: app: console-system sessionAffinity: None --- apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: system-gateway spec: selector: istio: ingressgateway # use Istio default gateway implementation servers: - port: number: 80 name: http protocol: HTTP hosts: - \"luban.system.galaxy.cloud\" --- apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: system spec: hosts: - \"luban.system.galaxy.cloud\" gateways: - system-gateway http: - match: - uri: prefix: / route: - destination: port: number: 80 host: console-system corsPolicy: allowOrigins: - exact: \"*\" allowMethods: - GET - POST - PATCH - PUT - DELETE - OPTIONS allowCredentials: false allowHeaders: - authorization maxAge: \"24h\" \u5b89\u88c5\u9c81\u73edMonitor --- apiVersion: apps/v1 kind: Deployment metadata: labels: app: console-monitor name: console-monitor namespace: luban-fe spec: progressDeadlineSeconds: 600 replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: app: console-monitor strategy: rollingUpdate: maxSurge: 25% maxUnavailable: 25% type: RollingUpdate template: metadata: creationTimestamp: null labels: app: console-monitor spec: containers: - image: harbor.inner.galaxy.ksyun.com/watt/luban-fe/console-monitor:latest imagePullPolicy: Always name: console-monitor resources: {} terminationMessagePath: /dev/termination-log terminationMessagePolicy: File dnsPolicy: ClusterFirst restartPolicy: Always schedulerName: default-scheduler securityContext: {} terminationGracePeriodSeconds: 30 --- apiVersion: v1 kind: Service metadata: labels: app: console-monitor name: console-monitor namespace: luban-fe spec: ports: - port: 80 protocol: TCP targetPort: 80 selector: app: console-monitor sessionAffinity: None --- apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: monitor-gateway spec: selector: istio: ingressgateway # use Istio default gateway implementation servers: - port: number: 80 name: http protocol: HTTP hosts: - \"luban.monitor.galaxy.cloud\" --- apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: monitor spec: hosts: - \"luban.monitor.galaxy.cloud\" gateways: - monitor-gateway http: - match: - uri: prefix: / route: - destination: port: number: 80 host: console-monitor corsPolicy: allowOrigins: - exact: \"*\" allowMethods: - GET - POST - PATCH - PUT - DELETE - OPTIONS allowCredentials: false allowHeaders: - authorization maxAge: \"24h\" \u5b89\u88c5\u9c81\u73edDemo --- apiVersion: apps/v1 kind: Deployment metadata: labels: app: console-demo name: console-demo namespace: luban-fe spec: progressDeadlineSeconds: 600 replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: app: console-demo strategy: rollingUpdate: maxSurge: 25% maxUnavailable: 25% type: RollingUpdate template: metadata: creationTimestamp: null labels: app: console-demo spec: containers: - image: harbor.inner.galaxy.ksyun.com/watt/luban-fe/console-demo:latest imagePullPolicy: Always name: console-demo resources: {} terminationMessagePath: /dev/termination-log terminationMessagePolicy: File dnsPolicy: ClusterFirst restartPolicy: Always schedulerName: default-scheduler securityContext: {} terminationGracePeriodSeconds: 30 --- apiVersion: v1 kind: Service metadata: labels: app: console-demo name: console-demo namespace: luban-fe spec: ports: - port: 80 protocol: TCP targetPort: 80 selector: app: console-demo sessionAffinity: None --- apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: demo-gateway spec: selector: istio: ingressgateway # use Istio default gateway implementation servers: - port: number: 80 name: http protocol: HTTP hosts: - \"luban.demo.galaxy.cloud\" --- apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: demo spec: hosts: - \"luban.demo.galaxy.cloud\" gateways: - demo-gateway http: - match: - uri: prefix: / route: - destination: port: number: 80 host: console-demo corsPolicy: allowOrigins: - exact: \"*\" allowMethods: - GET - POST - PATCH - PUT - DELETE - OPTIONS allowCredentials: false allowHeaders: - authorization maxAge: \"24h\" \u5b89\u88c5\u547d\u4ee4 #!/bin/sh kubectl apply -f base.yaml -f demo.yaml -f monitor.yaml -f system.yaml -n luban-fe \u64cd\u4f5c\u624b\u518c \u76d1\u63a7 \u914d\u7f6eprometheus\u91c7\u96c6 \u76ee\u524d\u7814\u53d1\u9636\u6bb5\uff0c\u9700\u8981\u624b\u52a8\u914d\u7f6eprometheus\u91c7\u96c6\uff0c\u521d\u6b21\u521b\u5efa\u7684\u65f6\u5019\uff0c\u9700\u8981\u5728k8s\u4e2d\u521b\u5efaServiceMonitor\u7684crd\u8d44\u6e90\uff0c\u540e\u7eed\u4f1a\u914d\u5408cmdb\u505a\u670d\u52a1\u81ea\u52a8\u53d1\u73b0\uff0c\u6ce8\u518c\u5230k8s\u7684endpoint\u3002 \u9996\u6b21\u914d\u7f6e\u91c7\u96c6\u6267\u884c\u4ee5\u4e0b\u811a\u672c\uff1a cat <<EOF | kubectl -n monitoring apply -f - kind: Service apiVersion: v1 metadata: labels: k8s-app: external-nodes name: external-nodes spec: type: ClusterIP ports: - name: metrics port: 9100 targetPort: 9100 --- kind: Endpoints apiVersion: v1 metadata: labels: k8s-app: external-nodes name: external-nodes subsets: - addresses: - ip: 10.177.9.11 ports: - name: metrics port: 9100 --- apiVersion: monitoring.coreos.com/v1 kind: ServiceMonitor metadata: labels: k8s-app: external-nodes name: external-nodes spec: endpoints: - interval: 60s port: metrics namespaceSelector: matchNames: - monitoring selector: matchLabels: k8s-app: external-nodes EOF kubectl label servicemonitor external-nodes release=pm -n monitoring \u5982\u679c\u540e\u7eed\u9700\u8981\u589e\u52a0\u76d1\u63a7\u8d44\u6e90\uff0c\u9700\u8981\u4fee\u6539\u5df2\u7ecf\u521b\u5efa\u7684endpoint\uff0c\u767b\u5f55\u9c81\u73ed\u7684Master\u8282\u70b9\uff0c\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff1a kubectl edit ep external-nodes -n monitoring ## \u589e\u52a0\u6216\u4fee\u6539subsets\u5b57\u6bb5\uff0c\u6ce8\u610fyaml\u683c\u5f0f \u767b\u5f55prometheus\u754c\u9762\u67e5\u770b\u65f6\u5019\u6dfb\u52a0\u6210\u529ftarget \uff1a http://10.177.152.168:9090/targets\uff0c\u641c\u7d22IP\uff0c\u72b6\u6001\u4e3aup \u767b\u5f55graph\uff0c\u67e5\u770b\u76d1\u63a7\uff1ahttp://10.177.152.168:9090/graph?g0.expr=crash_check&g0.tab=0&g0.stacked=0&g0.range_input=1h \u8f93\u5165\u76d1\u63a7\u9879\uff1a\u6bd4\u5982crash_check \u652f\u6301label\u67e5\u8be2\uff0c\u53c2\u7167wiki\uff1a PromQL \u914d\u7f6e\u6570\u636e\u5b58\u653e\u65f6\u95f4 kubectl edit prometheuses.monitoring.coreos.com -n monitoring pm-kube-prometheus-stack-prometheus -o yaml \u544a\u8b66 \u914d\u7f6e\u544a\u8b66 \u76ee\u524dprometheus\u90e8\u7f72\u65b9\u5f0f\u91c7\u7528operator\u90e8\u7f72\uff0c\u521b\u5efa\u544a\u8b66\u7b56\u7565PrometheusRule cat <<EOF | kubectl -n monitoring apply -f - apiVersion: monitoring.coreos.com/v1 kind: PrometheusRule metadata: labels: app: kube-prometheus-stack release: pm name: test-alert-rules namespace: monitoring spec: groups: - name: test rules: - alert: customize-webhook-rule annotations: summary: summary '{{ $labels.target }}' crash_check description: description '{{ $labels.target }}' crash_check message: message '{{ $labels.target }}' crash_check expr: | crash_check <10 for: 1m labels: severity: critical alertTag: vip EOF \u544a\u8b66\u89c4\u5219\u4e5f\u652f\u6301prometheus\u7684\u5404\u79cdlabel\u4ee5\u53ca\u8868\u8fbe\u5f0f\uff0c\u53c2\u7167wiki\uff1a PromQL \u767b\u5f55prometheus\u754c\u9762\u67e5\u770brule\u662f\u5426\u521b\u5efa http://10.177.152.168:9090/rules \u544a\u8b66\u89c4\u5219\u64cd\u4f5c \u5982\u679c\u5bf9\u5df2\u7ecf\u521b\u5efa\u7684\u544a\u8b66\u89c4\u5219\u8fdb\u884c\u4fee\u6539\uff0c\u767b\u5f55kubectl\u547d\u4ee4\u64cd\u4f5ck8s\u7684\u4e2dPrometheusRule\u8d44\u6e90\uff0c\u547d\u540d\u7a7a\u95f4\u4e3amonitoring ## \u83b7\u53d6\u5168\u90e8\u544a\u8b66\u89c4\u5219 kubectl get PrometheusRule -n monitoring ## \u4fee\u6539\u544a\u8b66\u89c4\u5219 kubectl edit PrometheusRule {RuleName} -n monitoring \u67e5\u770b\u544a\u8b66 \u767b\u5f55alertmanager\u754c\u9762 http://10.177.152.168:9093/#/alerts\u67e5\u770b\u544a\u8b66 \u767b\u5f55elasticsearch\u754c\u9762[http://10.177.152.168:9601\uff0c\u67e5\u770b\u544a\u8b66\u5386\u53f2\uff0cindex\u4e3aalertmanager* \u914d\u7f6eWebHook \u76ee\u524d\u9752\u5c9b5\u73af\u5883\u5df2\u7ecf\u914d\u7f6e2\u4e2awebhook\uff0c\u4e00\u4e2a\u5199\u5165elasticsearch\uff0c\u7528\u4e8e\u544a\u8b66\u5386\u53f2\u67e5\u8be2\uff1b\u4e00\u4e2a\u5199\u5165\u98de\u4e66\uff0c\u7528\u4e8e\u8fd0\u7ef4\u4eba\u5458\u5b9e\u65f6\u5904\u7406\u3002 \u914d\u7f6e\u547d\u4ee4\u5982\u4e0b kubectl create deploy alertmanager-warning --image=harbor.inner.galaxy.ksyun.com/luban/alertmanager-output -n monitoring kubectl create deploy alertmanager-feishu --image=harbor.inner.galaxy.ksyun.com/luban/feishu-webhook:latest -n monitoring kubectl expose deploy/alertmanager-warning -n monitoring --port=8080 --target-port=8080 kubectl expose deploy/alertmanager-feishu -n monitoring --port=8080 --target-port=8080 cat <<EOF | kubectl -n monitoring apply -f - apiVersion: monitoring.coreos.com/v1alpha1 kind: AlertmanagerConfig metadata: name: warning labels: alertmanagerConfig: warning release: pm spec: route: groupBy: ['alertname'] groupWait: 30s groupInterval: 1m repeatInterval: 1m receiver: 'warning-hook' receivers: - name: 'warning-hook' webhookConfigs: - url: 'http://alertmanager-warning:8080/webhook' --- apiVersion: monitoring.coreos.com/v1alpha1 kind: AlertmanagerConfig metadata: name: feishu labels: alertmanagerConfig: feshu release: pm spec: route: groupBy: ['alertname'] groupWait: 30s groupInterval: 1m repeatInterval: 1m receiver: 'feishu-hook' receivers: - name: 'feishu-hook' webhookConfigs: - url: 'http://alertmanager-feishu:8080/alertmanager-alert' EOF \u65e5\u5fd7 \u914d\u7f6efilebeat \u4fee\u6539/etc/filebeat/filebeat.yml\uff0c\u6839\u636e\u4e0d\u540c\u65e5\u5fd7\uff0c\u4f20\u5165kafka\u4e0d\u540ctopic\u4e2d logging.level: debug filebeat.inputs: # \u4ece\u8fd9\u91cc\u5f00\u59cb\u5b9a\u4e49\u6bcf\u4e2a\u65e5\u5fd7\u7684\u8def\u5f84\u3001\u7c7b\u578b\u3001\u6536\u96c6\u65b9\u5f0f\u7b49\u4fe1\u606f - type: log # \u6307\u5b9a\u6536\u96c6\u7684\u7c7b\u578b\u4e3a log paths: - /var/log/secure # \u8bbe\u7f6e access.log \u7684\u8def\u5f84 fields: # \u8bbe\u7f6e\u4e00\u4e2a fields\uff0c\u7528\u4e8e\u6807\u8bb0\u8fd9\u4e2a\u65e5\u5fd7 log_topic: topic-for-secure # \u4e3a fields \u8bbe\u7f6e\u4e00\u4e2a\u5173\u952e\u5b57 topic\uff0c\u503c\u4e3a kafka \u4e2d\u5df2\u7ecf\u8bbe\u7f6e\u597d\u7684 topic \u540d\u79f0 - type: log paths: - /var/log/messages # \u8bbe\u7f6e info.log \u7684\u8def\u5f84 fields: # \u8bbe\u7f6e\u4e00\u4e2a fields\uff0c\u7528\u4e8e\u6807\u8bb0\u8fd9\u4e2a\u65e5\u5fd7 log_topic: topic-for-messages # \u4e3a fields \u8bbe\u7f6e\u4e00\u4e2a\u5173\u952e\u5b57 topic\uff0c\u503c\u4e3a kafka \u4e2d\u5df2\u7ecf\u8bbe\u7f6e\u597d\u7684 topic \u540d\u79f0 output.kafka: # \u662f\u5426\u542f\u52a8 enable: true hosts: [\"10.177.152.168:30092\",\"10.177.152.168:30093\",\"10.177.152.168:30094\"] partition.round_robin: #\u5f00\u542fkafka\u7684partition\u5206\u533a reachable_only: true worker: 2 # \u4ee3\u7406\u8981\u6c42\u7684ACK\u53ef\u9760\u6027\u7ea7\u522b # 0=\u65e0\u54cd\u5e94\uff0c1=\u7b49\u5f85\u672c\u5730\u63d0\u4ea4\uff0c-1=\u7b49\u5f85\u6240\u6709\u526f\u672c\u63d0\u4ea4 # \u9ed8\u8ba4\u503c\u662f1 # \u6ce8\u610f:\u5982\u679c\u8bbe\u7f6e\u4e3a0,Kafka\u4e0d\u4f1a\u8fd4\u56de\u4efb\u4f55ack\u3002\u51fa\u9519\u65f6\uff0c\u6d88\u606f\u53ef\u80fd\u4f1a\u6084\u65e0\u58f0\u606f\u5730\u4e22\u5931\u3002 required_acks: 1 compression: gzip #\u538b\u7f29\u683c\u5f0f max_message_bytes: 10000000 #\u538b\u7f29\u683c\u5f0f\u5b57\u8282\u5927\u5c0f # topic: '%{[fields.topic]}' # \u6839\u636e\u6bcf\u4e2a\u65e5\u5fd7\u8bbe\u7f6e\u7684 fields.topic \u6765\u8f93\u51fa\u5230\u4e0d\u540c\u7684 topic topic: '%{[fields.log_topic]}' \u914d\u7f6elogstash \u5728logging\u7684\u5b89\u88c5\u76ee\u5f55\uff0c\u6839\u636e\u4e0d\u540c\u65e5\u5fd7\uff0c\u83b7\u53d6kafka\u4e0d\u540ctopic\u4e2d,\u4fee\u6539values.yaml logstashConfig: logstash.yml: | http.host: 0.0.0.0 monitoring.elasticsearch.hosts: http://elasticsearch-master:9200 # Allows you to add any pipeline files in /usr/share/logstash/pipeline/ ### ***warn*** there is a hardcoded logstash.conf in the image, override it first logstashPipeline: logstash.conf: | input { kafka { bootstrap_servers => \"kafka-0.kafka-headless.elastic-system.svc.cluster.local:9092,kafka-1.kafka-headless.elastic-system.svc.cluster.local:9092,kafka-2.kafka-headless.elastic-system.svc.cluster.local:9092\" topics => [\"topic-for-secure\"] } } output { elasticsearch { hosts => [\"http://elasticsearch-master:9200\"] index => \"topic-for-secure-%{+YYYY.MM.dd}\" } } # logstash.conf: | # input { # exec { # command => \"uptime\" # interval => 30 # } # } # output { stdout { } } \u5728k8s\u4e2d\u521b\u5efa\u65b0\u7684logstash kubectl create ns elastic-system export REGISTRY=\"harbor.inner.galaxy.ksyun.com\" helm install {logstash--new-name} ./logstash -n elastic-system --set imageTag=7.10.1 --set image=$REGISTRY/luban/logstash \u914d\u7f6ekibana \u767b\u5f55kibana\u754c\u9762\uff0c\u67e5\u770bes\u4e2d\u662f\u5426\u521b\u5efa\u65b0\u7684index http://10.177.152.168:9601/app/management/data/index_management/indices \u521b\u5efakibana\u7684indexPatterns http://10.177.152.168:9601/app/management/kibana/indexPatterns \u914d\u7f6e\u65e5\u5fd7\u6e05\u7406 \u9700\u8981\u6ce8\u610f\uff0ces\u4e2d\u7684index\u9700\u8981\u6709@timestamp \u5728logging\u7684\u5b89\u88c5\u76ee\u5f55\uff0c\u4fee\u6539 kk get cm -n elastic-system curator-elasticsearch-curator-config -o yaml \u5b58\u50a8\u5bb9\u91cf\u89c4\u5212 \u76d1\u63a7 \u78c1\u76d8\u5bb9\u91cf\u89c4\u5212\uff0c\u9996\u5148\u660e\u786e\u51e0\u4e2a\u6982\u5ff5: \u76d1\u63a7\u8282\u70b9 : \u4e00\u4e2a exporter \u8fdb\u7a0b\u88ab\u8ba4\u4e3a\u662f\u4e00\u4e2a\u76d1\u63a7\u8282\u70b9\u3002Manager \u5728\u5b89\u88c5 AQUILA\u65f6\uff0c\u9ed8\u8ba4\u6bcf\u4e2a\u8282\u70b9\u90fd\u4f1a\u5b89\u88c5\u4e00\u4e2a node-exporter \u6536\u96c6\u8282\u70b9\u4fe1\u606f(CPU, Memory \u7b49), \u6bcf\u4e2a\u8282\u70b9\u5b89\u88c5\u4e00\u4e2a tdh-exporter \u6536\u96c6 TDH Services metrics (\u76ee\u524d\u6709: HDFS, YARN, ZOOKEEPER, KAFKA, HYPERBASE, INCEPTOR). \u6545\u5728 TDH \u96c6\u7fa4\u4e0a, \u6bcf\u4e2a\u8282\u70b9\u6709\u4e24\u4e2a exporter \u8fdb\u7a0b. \u6d4b\u91cf\u70b9 : \u4e00\u4e2a\u6d4b\u91cf\u70b9\u4ee3\u8868\u4e86\u67d0\u76d1\u63a7\u8282\u70b9\u4e0a\u7684\u4e00\u4e2a\u89c2\u6d4b\u5bf9\u8c61. \u4ece\u67d0\u6d4b\u91cf\u70b9\u91c7\u96c6\u5230\u7684\u4e00\u7ec4\u6837\u672c\u6570\u636e\u6784\u6210\u4e00\u6761\u65f6\u95f4\u5e8f\u5217\uff08time series). \u6293\u53d6\u95f4\u9694 : Promtheus \u5bf9\u67d0\u4e2a\u76d1\u63a7\u8282\u70b9\u91c7\u96c6 metrics \u7684\u65f6\u95f4\u95f4\u9694. \u4e00\u822c\u4e3a\u540c\u7c7b\u76d1\u63a7\u8282\u70b9\u8bbe\u7f6e\u76f8\u540c\u7684\u6293\u53d6\u95f4\u9694. AQUILA\u5bf9\u5e94\u7684\u914d\u7f6e\u503c\u4e3a: prometheus.node.exporter.scrape_interval(\u9ed8\u8ba415s) \u548c prometheus.tdh.exporter.scrape_interval(\u9ed8\u8ba460s) \u4fdd\u7559\u65f6\u95f4 : \u6837\u672c\u6570\u636e\u5728\u78c1\u76d8\u4e0a\u4fdd\u5b58\u7684\u65f6\u95f4,\u8d85\u8fc7\u8be5\u65f6\u95f4\u9650\u5236\u7684\u6570\u636e\u5c31\u4f1a\u88ab\u5220\u9664. \u5b58\u50a8\u5728\u78c1\u76d8\u4e0a\u7684\u6837\u672c\u90fd\u662f\u7ecf\u8fc7\u7f16\u7801\u4e4b\u540e\u7684\u6837\u672c(\u5bf9\u6837\u672c\u8fdb\u884c\u8fc7\u6570\u636e\u7f16\u7801, \u4e00\u822c\u4e3a double-delta \u7f16\u7801). AQUILA\u5bf9\u5e94\u7684\u914d\u7f6e\u503c\u4e3a prometheus.storage.retention.time(\u9ed8\u8ba415\u5929) \u6d3b\u8dc3\u6837\u672c\u7559\u5b58\u65f6\u95f4 : \u7559\u5b58\u4e8e\u5185\u5b58\u7684\u6d3b\u8dc3\u6837\u672c\uff08\u5df2\u7ecf\u88ab\u7f16\u7801\uff09\u5728\u5185\u5b58\u4fdd\u7559\u65f6\u95f4. \u5728\u5185\u5b58\u4e2d\u7684\u7559\u5b58\u6570\u636e\u8d8a\u591a\uff0c\u67e5\u8be2\u8fc7\u5f80\u6570\u636e\u7684\u6027\u80fd\u8d8a\u9ad8\uff0c\u4f46\u662f\u6d88\u8017\u5185\u5b58\u4e5f\u4f1a\u589e\u52a0. \u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u9700\u8981\u6839\u636e\u6240\u76d1\u63a7\u7684\u4e1a\u52a1\u7684\u6027\u8d28\uff0c\u8bbe\u5b9a\u5408\u7406\u7684\u5185\u5b58\u7559\u5b58\u65f6\u95f4. AQUILA\u5bf9\u5e94\u7684\u914d\u7f6e\u503c\u4e3a prometheus.min-block-duration (\u9ed8\u8ba42h), prometheus.max-block-duration(\u9ed8\u8ba426h). Facebook \u5728\u8bba\u6587 \u300aGorilla: A Fast, Scalable, In-Memory Time Series DataBase\u300b \uff08Prometheus\u5b9e\u73b0\u53c2\u8003\u8bba\u6587\uff09\u4e2d\u7ed9\u51fa\u4e86\u7559\u5b58\u5185\u5b58\u65f6\u95f4\u7684\u4e00\u822c\u7ecf\u9a8c: 26 h. \u6837\u672c(\u6d4b\u91cf\u70b9)\u5927\u5c0f : \u6839\u636e Prometheus \u5b98\u65b9\u6587\u6863\u8bf4\u660e, \u6bcf\u4e00\u4e2a\u7f16\u7801\u540e\u7684\u6837\u672c\u5927\u6982\u5360\u75281-2\u5b57\u8282\u5927\u5c0f \u65e5\u5fd7 \u9700\u8981\u6839\u636e\u4e0d\u540c\u7684\u65e5\u5fd7\uff0c\u4ee5\u53ca\u65e5\u5fd7\u7684\u5927\u5c0f\u89c4\u5212 \u767b\u5f55kibana\u67e5\u770bindex\u4f7f\u7528\u5927\u5c0f\uff0c\u89c4\u5212\u65e5\u5fd7\u7684\u5b58\u50a8 http://10.177.152.168:9601/app/management/data/index_management/indices \u9644\u5f55 \u9752\u5c9b1\u53d1\u5e03\u73af\u5883hosts\u4fe1\u606f https://wiki.op.ksyun.com/pages/viewpage.action?pageId=157002294 \u544a\u8b66\u4ea7\u751f\u6d41\u7a0b Prometheus Server\u76d1\u63a7\u76ee\u6807\u4e3b\u673a\u4e0a\u66b4\u9732\u7684http\u63a5\u53e3\uff08\u8fd9\u91cc\u5047\u8bbe\u63a5\u53e3A\uff09\uff0c\u901a\u8fc7\u4e0a\u8ff0Promethes\u914d\u7f6e\u7684'scrape_interval'\u5b9a\u4e49\u7684\u65f6\u95f4\u95f4\u9694\uff0c\u5b9a\u671f\u91c7\u96c6\u76ee\u6807\u4e3b\u673a\u4e0a\u76d1\u63a7\u6570\u636e\u3002 \u5f53\u63a5\u53e3A\u4e0d\u53ef\u7528\u7684\u65f6\u5019\uff0cServer\u7aef\u4f1a\u6301\u7eed\u7684\u5c1d\u8bd5\u4ece\u63a5\u53e3\u4e2d\u53d6\u6570\u636e\uff0c\u76f4\u5230\"scrape_timeout\"\u65f6\u95f4\u540e\u505c\u6b62\u5c1d\u8bd5\u3002\u8fd9\u65f6\u5019\u628a\u63a5\u53e3\u7684\u72b6\u6001\u53d8\u4e3a\u201cDOWN\u201d\u3002 Prometheus\u540c\u65f6\u6839\u636e\u914d\u7f6e\u7684\"evaluation_interval\"\u7684\u65f6\u95f4\u95f4\u9694\uff0c\u5b9a\u671f\uff08\u9ed8\u8ba41min\uff09\u7684\u5bf9Alert Rule\u8fdb\u884c\u8bc4\u4f30\uff1b\u5f53\u5230\u8fbe\u8bc4\u4f30\u5468\u671f\u7684\u65f6\u5019\uff0c\u53d1\u73b0\u63a5\u53e3A\u4e3aDOWN\uff0c\u5373UP=0\u4e3a\u771f\uff0c\u6fc0\u6d3bAlert\uff0c\u8fdb\u5165\u201cPENDING\u201d\u72b6\u6001\uff0c\u5e76\u8bb0\u5f55\u5f53\u524dactive\u7684\u65f6\u95f4\uff1b \u5f53\u4e0b\u4e00\u4e2aalert rule\u7684\u8bc4\u4f30\u5468\u671f\u5230\u6765\u7684\u65f6\u5019\uff0c\u53d1\u73b0UP=0\u7ee7\u7eed\u4e3a\u771f\uff0c\u7136\u540e\u5224\u65ad\u8b66\u62a5Active\u7684\u65f6\u95f4\u662f\u5426\u5df2\u7ecf\u8d85\u51farule\u91cc\u7684\u2018for\u2019 \u6301\u7eed\u65f6\u95f4\uff0c\u5982\u679c\u672a\u8d85\u51fa\uff0c\u5219\u8fdb\u5165\u4e0b\u4e00\u4e2a\u8bc4\u4f30\u5468\u671f\uff1b\u5982\u679c\u65f6\u95f4\u8d85\u51fa\uff0c\u5219alert\u7684\u72b6\u6001\u53d8\u4e3a\u201cFIRING\u201d\uff1b\u540c\u65f6\u8c03\u7528Alertmanager\u63a5\u53e3\uff0c\u53d1\u9001\u76f8\u5173\u62a5\u8b66\u6570\u636e\u3002 AlertManager\u6536\u5230\u62a5\u8b66\u6570\u636e\u540e\uff0c\u4f1a\u5c06\u8b66\u62a5\u4fe1\u606f\u8fdb\u884c\u5206\u7ec4\uff0c\u7136\u540e\u6839\u636ealertmanager\u914d\u7f6e\u7684\u201cgroup_wait\u201d\u65f6\u95f4\u5148\u8fdb\u884c\u7b49\u5f85\u3002\u7b49wait\u65f6\u95f4\u8fc7\u540e\u518d\u53d1\u9001\u62a5\u8b66\u4fe1\u606f\u3002 \u5c5e\u4e8e\u540c\u4e00\u4e2aAlert Group\u7684\u8b66\u62a5\uff0c\u5728\u7b49\u5f85\u7684\u8fc7\u7a0b\u4e2d\u53ef\u80fd\u8fdb\u5165\u65b0\u7684alert\uff0c\u5982\u679c\u4e4b\u524d\u7684\u62a5\u8b66\u5df2\u7ecf\u6210\u529f\u53d1\u51fa\uff0c\u90a3\u4e48\u95f4\u9694\u201cgroup_interval\u201d\u7684\u65f6\u95f4\u95f4\u9694\u540e\u518d\u91cd\u65b0\u53d1\u9001\u62a5\u8b66\u4fe1\u606f\u3002\u6bd4\u5982\u914d\u7f6e\u7684\u662f\u90ae\u4ef6\u62a5\u8b66\uff0c\u90a3\u4e48\u540c\u5c5e\u4e00\u4e2agroup\u7684\u62a5\u8b66\u4fe1\u606f\u4f1a\u6c47\u603b\u5728\u4e00\u4e2a\u90ae\u4ef6\u91cc\u8fdb\u884c\u53d1\u9001\u3002 \u5982\u679cAlert Group\u91cc\u7684\u8b66\u62a5\u4e00\u76f4\u6ca1\u53d1\u751f\u53d8\u5316\u5e76\u4e14\u5df2\u7ecf\u6210\u529f\u53d1\u9001\uff0c\u7b49\u5f85\u2018repeat_interval\u2019\u65f6\u95f4\u95f4\u9694\u4e4b\u540e\u518d\u91cd\u590d\u53d1\u9001\u76f8\u540c\u7684\u62a5\u8b66\u90ae\u4ef6\uff1b\u5982\u679c\u4e4b\u524d\u7684\u8b66\u62a5\u6ca1\u6709\u6210\u529f\u53d1\u9001\uff0c\u5219\u76f8\u5f53\u4e8e\u89e6\u53d1\u7b2c6\u6761\u6761\u4ef6\uff0c\u5219\u9700\u8981\u7b49\u5f85group_interval\u65f6\u95f4\u95f4\u9694\u540e\u91cd\u590d\u53d1\u9001\u3002 \u540c\u65f6\u6700\u540e\u81f3\u4e8e\u8b66\u62a5\u4fe1\u606f\u5177\u4f53\u53d1\u7ed9\u8c01\uff0c\u6ee1\u8db3\u4ec0\u4e48\u6837\u7684\u6761\u4ef6\u4e0b\u6307\u5b9a\u8b66\u62a5\u63a5\u6536\u4eba\uff0c\u8bbe\u7f6e\u4e0d\u540c\u62a5\u8b66\u53d1\u9001\u9891\u7387\uff0c\u8fd9\u91cc\u6709alertmanager\u7684route\u8def\u7531\u89c4\u5219\u8fdb\u884c\u914d\u7f6e\u3002","title":"\u4ecb\u7ecd"},{"location":"#_1","text":"\u7565","title":"\u4ecb\u7ecd"},{"location":"#_2","text":"\u76ee\u524d\u9c81\u73ed\u4ea7\u54c1\u91c7\u7528Prometheus\u7684\u6574\u4f53\u67b6\u6784\uff0c\u540c\u65f6\u4e5f\u53c2\u7167kingsoft cloud\u7684eagles\u76d1\u63a7\u67b6\u6784\u4fee\u6539\uff0ceagles\u76ee\u524d\u91c7\u7528\u67b6\u6784\u4e3a\uff1a prometheus\u7684\u539f\u67b6\u6784\uff1a \u76ee\u524d\u9c81\u73ed\u67b6\u6784\u589e\u52a0luban_agent\u63a5\u53d7\u539f\u6709\u7684eagles\u7684\u81ea\u5b9a\u4e49\u76d1\u63a7\u811a\u672c\uff0c\u589e\u52a0node_exporter\u7269\u7406\u673a\u76d1\u63a7\uff0c\u589e\u52a0\u544a\u8b66\u5386\u53f2\u5b58\u50a8\u5230elasticsearch\u4ee5\u53ca\u5b9e\u65f6\u544a\u8b66\u5904\u7406\u63a8\u9001\u98de\u4e66\uff0c\u5177\u4f53\u67b6\u6784\u5982\u4e0b\uff1a","title":"\u76d1\u63a7\u544a\u8b66\u67b6\u6784"},{"location":"#_3","text":"\u5728\u65e5\u5fd7\u6536\u96c6\u4e2d\uff0c\u90fd\u662f\u4f7f\u7528\u7684filebeat+ELK\u7684\u65e5\u5fd7\u67b6\u6784\u3002\u4f46\u662f\u5982\u679c\u4e1a\u52a1\u6bcf\u5929\u4f1a\u4ea7\u751f\u6d77\u91cf\u7684\u65e5\u5fd7\uff0c\u5c31\u6709\u53ef\u80fd\u5f15\u53d1logstash\u548celasticsearch\u7684\u6027\u80fd\u74f6\u9888\u95ee\u9898\u3002\u56e0\u6b64\u6539\u5584\u8fd9\u4e00\u95ee\u9898\u7684\u65b9\u6cd5\u5c31\u662ffilebeat+kafka+logstash+ELK\uff0c \u4e5f\u5c31\u662f\u5c06\u5b58\u50a8\u4eceelasticsearch\u8f6c\u79fb\u7ed9\u6d88\u606f\u4e2d\u95f4\u4ef6\uff0c\u51cf\u5c11\u6d77\u91cf\u6570\u636e\u5f15\u8d77\u7684\u5b95\u673a\uff0c\u964d\u4f4eelasticsearch\u7684\u538b\u529b\uff0c\u8fd9\u91cc\u7684elasticsearch\u4e3b\u8981\u8fdb\u884c\u6570\u636e\u7684\u5206\u6790\u5904\u7406\uff0c\u7136\u540e\u4ea4\u7ed9kibana\u8fdb\u884c\u754c\u9762\u5c55\u793a","title":"\u65e5\u5fd7\u67b6\u6784"},{"location":"#cmdb","text":"Dgraph\u7ec4\u4ef6\u5305\u62ec\u4e09\u4e2a\u90e8\u5206\uff1a Zero: \u662f\u96c6\u7fa4\u7684\u6838\u5fc3, \u8d1f\u8d23\u8c03\u5ea6\u96c6\u7fa4\u670d\u52a1\u5668\u548c\u5e73\u8861\u670d\u52a1\u5668\u7ec4\u4e4b\u95f4\u7684\u6570\u636e\uff0c\u7c7b\u6bd4\u4e8eElasticsearch\u7684master\u8282\u70b9\uff1b Alpha: \u4fdd\u5b58\u6570\u636e\u7684 \u8c13\u8bcd \u548c \u7d22\u5f15. \u8c13\u8bcd\u5305\u62ec\u6570\u636e\u7684 \u5c5e\u6027 \u548c\u6570\u636e\u4e4b\u95f4\u7684 \u5173\u7cfb; \u7d22\u5f15\u662f\u4e3a\u4e86\u66f4\u5feb\u7684\u8fdb\u884c\u6570\u636e\u7684\u8fc7\u6ee4\u548c\u67e5\u627e\uff0c\u7c7b\u6bd4\u4e8eElasticsearch\u7684data\u8282\u70b9\uff1b Ratel: dgraph \u7684 UI \u63a5\u53e3, \u53ef\u4ee5\u5728\u6b64\u754c\u9762\u4e0a\u8fdb\u884c\u6570\u636e\u7684 CURD, \u4e5f\u53ef\u4ee5\u4fee\u6539\u6570\u636e\u7684 schema\uff0c\u7c7b\u6bd4\u4e8eElasticsearch\u7684kibana\u89d2\u8272","title":"CMDB\u67b6\u6784"},{"location":"#_4","text":"","title":"\u5b89\u88c5\u6b65\u9aa4"},{"location":"#_5","text":"\u76ee\u524d\u9c81\u73ed\u7684\u7ec4\u4ef6\uff08\u9664\u76d1\u63a7\u6570\u636e\u91c7\u96c6\u7aef\u7684node_exporter\u4ee5\u53caluban_agent\u662f\u90e8\u7f72\u5728\u7269\u7406\u673a\u670d\u52a1\u5668\u4e0a\u4e4b\u5916\uff0c\u5176\u4ed6\u7ec4\u4ef6\u5747\u90e8\u7f72\u5728kubernetes\u96c6\u7fa4\u4e2d\uff0c\u5b89\u88c5\u9c81\u73ed\u7ec4\u4ef6\u4e4b\u524d\u9700\u8981\u521d\u59cb\u5316\u4e00\u4e2a\u9ad8\u53ef\u7528\u7684kubernetes\u96c6\u7fa4\uff0c\u96c6\u7fa4\u6700\u4f4e\u914d\u7f6e\u5982\u4e0b\uff0c\u8ba1\u7b97\u8d44\u6e90\u4e0e\u5b58\u50a8\u8d44\u6e90\u6839\u636e\u76d1\u63a7\u6570\u636e\u91cf\u4ee5\u53ca\u5b58\u50a8\u6570\u636e\u91cf\u9002\u5f53\u52a0\u5927\u3002 \u89d2\u8272 \u670d\u52a1\u5668\u540d\u79f0 \u4e3b\u673a\u540d \u8ba1\u7b97\u8d44\u6e90 \u78c1\u76d8\uff08\u89c1\u5b58\u50a8\u8d44\u6e90\u89c4\u5212\uff09 \u63a8\u8350\u7f51\u5361 \u64cd\u4f5c\u7cfb\u7edf \u63cf\u8ff0 \u7f51\u7edc\u7c7b\u578b \u5907\u6ce8 \u7ba1\u7406\u8282\u70b9\uff083\u53f0\u505a\u9ad8\u53ef\u7528\uff0c\u865a\u62df\u673a\u6216\u8005\u7269\u7406\u673a\uff09 Master1 x.x.x.x 8C16G 1\u3001\u6bcf\u53f0\u865a\u673a\u7cfb\u7edf\u76d8\uff0c20G\uff0c \u5173\u95edswap 2\u3001\u6bcf\u53f0\u865a\u673a\u5355\u72ec\u52a0\u4e00\u5757\uff08\u88f8\uff09\u6570\u636e\u76d8\uff0c100-500G\uff0c\u4f5c\u4e3aDocker VG 3\u3001\u6bcf\u53f0\u865a\u673a\u5355\u72ec\u52a0\u4e00\u5757\u6570\u636e\u76d8\uff0c50G\uff0c\u4f5c\u4e3aETCD\u6570\u636e\u5b58\u50a8 \u6302\u8f7d\u70b9\uff1a/var/lib/etcd \u6587\u4ef6\u7cfb\u7edf\uff1a xfs \u6570\u91cf\uff1a1 Centos7.x \u7ba1\u7406\u8282\u70b9 \u7269\u7406\u7f51\u7edc\u6216VLAN\u7c7b\u578b\u7684\u865a\u62df\u7f51\u7edc \u7ba1\u7406\u8282\u70b9\uff083\u53f0\u505a\u9ad8\u53ef\u7528\uff0c\u865a\u62df\u673a\u6216\u8005\u7269\u7406\u673a\uff09 Master2 x.x.x.x 8C16G \u540c\u4e0a \u6570\u91cf\uff1a1 Centos7.x \u7ba1\u7406\u8282\u70b9 \u540c\u4e0a \u7ba1\u7406\u8282\u70b9\uff083\u53f0\u505a\u9ad8\u53ef\u7528\uff0c\u865a\u62df\u673a\u6216\u8005\u7269\u7406\u673a\uff09 Master3 x.x.x.x 8C16G \u540c\u4e0a \u6570\u91cf\uff1a1 Centos7.x \u7ba1\u7406\u8282\u70b9 \u540c\u4e0a \u8ba1\u7b97\u8282\u70b9 Worker1 x.x.x.x 8C32G 1\u3001\u6bcf\u53f0\u865a\u673a\u7cfb\u7edf\u76d8\uff0c20G\uff0c \u5173\u95edswap 2\u3001\u6bcf\u53f0\u865a\u673a\u5355\u72ec\u52a0\u4e00\u5757\uff08\u88f8\uff09\u6570\u636e\u76d8\uff0c100G-500G\uff0c\u4f5c\u4e3aDocker VG \u6570\u91cf\uff1a1 Centos7.x \u8ba1\u7b97\u8282\u70b9 \u540c\u4e0a \u8ba1\u7b97\u8282\u70b9 Worker2 x.x.x.x 8C32G \u540c\u4e0a \u6570\u91cf\uff1a1 Centos7.x \u8ba1\u7b97\u8282\u70b9 \u540c\u4e0a \u8ba1\u7b97\u8282\u70b9 Worker....... x.x.x.x 8C32G \u540c\u4e0a \u6570\u91cf\uff1a1 Centos7.x \u8ba1\u7b97\u8282\u70b9 \u540c\u4e0a \u955c\u50cf\u4ed3\u5e93\uff08\u865a\u62df\u673a\uff09 Registry1 x.x.x.x 4C8G \u540c\u4e0a \u6570\u91cf\uff1a1 Centos7.x \u4ed3\u5e93 \u540c\u4e0a \u955c\u50cf\u4ed3\u5e93\uff08\u865a\u62df\u673a\uff09 Registr2 x.x.x.x 4C8G \u540c\u4e0a \u6570\u91cf\uff1a1 Centos7.x \u4ed3\u5e93 \u540c\u4e0a MasterLb1 Master\u8282\u70b9\u8d1f\u8f7d\u5747\u8861 MasterLb1 x.x.x.x 4C8G \u540c\u4e0a \u6570\u91cf\uff1a1 Centos7.x \u8d1f\u8f7d\u5747\u8861 \u540c\u4e0a \u6216\u8005\u4f7f\u7528slb MasterLb2 Master\u8282\u70b9\u8d1f\u8f7d\u5747\u8861 MasterLb2 x.x.x.x 4C8G \u540c\u4e0a \u6570\u91cf\uff1a1 Centos7.x \u8d1f\u8f7d\u5747\u8861 \u540c\u4e0a \u6216\u8005\u4f7f\u7528slb Registry\u8282\u70b9\u8d1f\u8f7d\u5747\u8861 RouterLb1 x.x.x.x 4C8G \u540c\u4e0a \u6570\u91cf\uff1a1 Centos7.x \u8d1f\u8f7d\u5747\u8861 \u540c\u4e0a \u6216\u8005\u4f7f\u7528slb Registry\u8282\u70b9\u8d1f\u8f7d\u5747\u8861 RouterLb2 x.x.x.x 4C8G \u540c\u4e0a \u6570\u91cf\uff1a1 Centos7.x \u8d1f\u8f7d\u5747\u8861 \u540c\u4e0a \u6216\u8005\u4f7f\u7528slb","title":"\u73af\u5883\u51c6\u5907"},{"location":"#kubernetes","text":"","title":"\u5b89\u88c5Kubernetes"},{"location":"#_6","text":"\u4e3b\u673a\u540d IP\u5730\u5740 master0 192.168.0.2 master1 192.168.0.3 master2 192.168.0.4 node0 192.168.0.5 \u670d\u52a1\u5668\u5bc6\u7801\uff1a123456","title":"\u73af\u5883\u4fe1\u606f"},{"location":"#_7","text":"\u53ea\u9700\u8981\u51c6\u5907\u597d\u670d\u52a1\u5668\uff0c\u5728\u4efb\u610f\u4e00\u53f0\u670d\u52a1\u5668\u4e0a\u6267\u884c\u4e0b\u9762\u547d\u4ee4\u5373\u53ef # \u4e0b\u8f7d\u5e76\u5b89\u88c5sealos, sealos\u662f\u4e2agolang\u7684\u4e8c\u8fdb\u5236\u5de5\u5177\uff0c\u76f4\u63a5\u4e0b\u8f7d\u62f7\u8d1d\u5230bin\u76ee\u5f55\u5373\u53ef, release\u9875\u9762\u4e5f\u53ef\u4e0b\u8f7d wget -c https://sealyun.oss-cn-beijing.aliyuncs.com/latest/sealos && \\ chmod +x sealos && mv sealos /usr/bin # \u4e0b\u8f7d\u79bb\u7ebf\u8d44\u6e90\u5305 wget -c https://sealyun.oss-cn-beijing.aliyuncs.com/2fb10b1396f8c6674355fcc14a8cda7c-v1.20.0/kube1.20.0.tar.gz # \u5b89\u88c5\u4e00\u4e2a\u4e09master\u7684kubernetes\u96c6\u7fa4 $ sealos init --passwd '123456' \\ --master 192.168.0.2 --master 192.168.0.3 --master 192.168.0.4 \\ --node 192.168.0.5 \\ --pkg-url /root/kube1.20.0.tar.gz \\ --version v1.20.0 \u53c2\u6570\u542b\u4e49 \u53c2\u6570\u540d \u542b\u4e49 \u793a\u4f8b passwd \u670d\u52a1\u5668\u5bc6\u7801 123456 master k8s master\u8282\u70b9IP\u5730\u5740 192.168.0.2 node k8s node\u8282\u70b9IP\u5730\u5740 192.168.0.3 pkg-url \u79bb\u7ebf\u8d44\u6e90\u5305\u5730\u5740\uff0c\u652f\u6301\u4e0b\u8f7d\u5230\u672c\u5730\uff0c\u6216\u8005\u4e00\u4e2a\u8fdc\u7a0b\u5730\u5740 /root/kube1.20.0.tar.gz version \u8d44\u6e90\u5305 \u5bf9\u5e94\u7684\u7248\u672c v1.20.0 \u589e\u52a0master sealos join --master 192.168.0.6 --master 192.168.0.7 sealos join --master 192.168.0.6-192.168.0.9 # \u6216\u8005\u591a\u4e2a\u8fde\u7eedIP \u589e\u52a0node sealos join --node 192.168.0.6 --node 192.168.0.7 sealos join --node 192.168.0.6-192.168.0.9 # \u6216\u8005\u591a\u4e2a\u8fde\u7eedIP \u5220\u9664\u6307\u5b9amaster\u8282\u70b9 sealos clean --master 192.168.0.6 --master 192.168.0.7 sealos clean --master 192.168.0.6-192.168.0.9 # \u6216\u8005\u591a\u4e2a\u8fde\u7eedIP \u5220\u9664\u6307\u5b9anode\u8282\u70b9 sealos clean --node 192.168.0.6 --node 192.168.0.7 sealos clean --node 192.168.0.6-192.168.0.9 # \u6216\u8005\u591a\u4e2a\u8fde\u7eedIP \u6e05\u7406\u96c6\u7fa4 sealos clean --all \u5907\u4efd\u96c6\u7fa4 sealos etcd save","title":"\u9ad8\u53ef\u7528\u5b89\u88c5\u6559\u7a0b"},{"location":"#_8","text":"","title":"\u5b89\u88c5\u670d\u52a1\u7f51\u683c"},{"location":"#istio","text":"istio\u7528\u4e8e\u7ba1\u7406\u94f6\u6cb3\u7684\u670d\u52a1\uff0c\u670d\u52a1\u4e4b\u95f4\u7684\u6743\u9650\u8bbe\u7f6e\u3002\u4e0b\u8f7distioctl\u5230\u5b89\u88c5\u73af\u5883 \u6267\u884c\u5b89\u88c5\u547d\u4ee4 \u79bb\u7ebf\u955c\u50cf\u5217\u8868 docker.io/istio/pilot:1.9.1 docker.io/istio/proxyv2:1.9.1 \u540c\u6b65\u955c\u50cf for i in `cat image.txt`;do docker pull $i;done for i in `cat image.txt`;do docker tag $i harbor.inner.galaxy.ksyun.com/istio/${i##*/};done for i in `cat image.txt`;do docker push harbor.inner.galaxy.ksyun.com/istio/${i##*/};done \u5b89\u88c5 istioctl install --set profile=demo -y --set values.global.hub=\"harbor.inner.galaxy.ksyun.com/istio\"","title":"\u5b89\u88c5Istio"},{"location":"#_9","text":"","title":"\u5b89\u88c5\u76d1\u63a7\u544a\u8b66\u7ec4\u4ef6"},{"location":"#prometheusalertmanager","text":"kubectl create ns monitoring rm -rf custom-values.yaml export REGISTRY=\"harbor.inner.galaxy.ksyun.com\" cat <<EOF > custom-values.yaml prometheusOperator: image: repository: $REGISTRY/luban/prometheus-operator tag: v0.45.0 prometheusConfigReloaderImage: repository: $REGISTRY/luban/prometheus-config-reloader tag: v0.45.0 admissionWebhooks: patch: image: repository: $REGISTRY/luban/kube-webhook-certgen tag: v1.5.0 alertmanager: alertmanagerSpec: image: repository: $REGISTRY/luban/alertmanager tag: v0.21.0 config: global: resolve_timeout: 5m route: group_by: ['alertname'] group_wait: 30s group_interval: 1m repeat_interval: 1m prometheus: prometheusSpec: image: repository: $REGISTRY/luban/prometheus tag: v2.24.0 kube-state-metrics: image: repository: $REGISTRY/luban/kube-state-metrics tag: v1.9.8 grafana: image: repository: $REGISTRY/luban/grafana tag: 7.4.2 sidecar: image: repository: $REGISTRY/luban/k8s-sidecar tag: 1.10.6 adminPassword: Kingsoft123 prometheus-node-exporter: image: repository: $REGISTRY/luban/node-exporter tag: v1.0.1 EOF helm install pm ./kube-prometheus-stack-13.13.1.tgz -n monitoring -f custom-values.yaml","title":"\u5b89\u88c5Prometheus+Alertmanager"},{"location":"#agent","text":"\u76ee\u524d\u9c81\u73ed\u7684\u7248\u672c\u53d1\u5e03\u653e\u5728\u9752\u5c9b5\u670d\u52a1\u5668\uff0c\u540e\u7eed\u4f1a\u8ddf\u94f6\u6cb3\u4ea7\u54c1\u4e00\u4e0b\u6253\u5305\u53d1\u5e03\u3002 centos65\u7248\u672c\u7269\u7406\u673a\u670d\u52a1\u5668 #!/bin/sh export agent_http_server=\"http://10.177.152.168:8888/luban/luban_on_k8s/agent/luban_agent\" export bin_dir=\"/opt/luban/bin\" mkdir -p $bin_dir ps -aux |grep luban_agent|grep -v grep|awk '{print $2}'|xargs kill -9 rm -rf $bin_dir/luban_agent curl -o $bin_dir/luban_agent $agent_http_server chmod a+x $bin_dir/luban_agent rm -rf /var/log/luban_agent.log cat > /opt/luban/bin/start_agent.sh << EOF nohup $bin_dir/luban_agent >/var/log/luban_agent.log 2>&1 & EOF chmod a+x /opt/luban/bin/start_agent.sh cat > /opt/luban/bin/stop_agent.sh << EOF ps -aux |grep luban_agent|grep -v grep|awk '{print $2}'|xargs kill -9 EOF chmod a+x /opt/luban/bin/stop_agent.sh cat > /etc/init.d/luban_agent << EOF #!/bin/bash # # /etc/rc.d/init.d/luban_agent # # Luban Agent # # description: Luban Agent # processname: luban_agent # chkconfig: - 85 15 # Source function library. . /etc/rc.d/init.d/functions PROGNAME=luban_agent PROG=/opt/luban/bin/\\$PROGNAME USER=root LOGFILE=/var/log/luban_agent.log LOCKFILE=/var/run/\\$PROGNAME.pid start() { echo -n \"Starting \\$PROGNAME: \" daemon --user $USER --pidfile=\"\\$LOCKFILE\" \"\\$PROG &>\\$LOGFILE &\" echo } stop() { echo -n \"Shutting down $PROGNAME: \" kill -9 \\`pidof \\$PROGNAME\\` rm -f $LOCKFILE echo } case \"\\$1\" in start) start ;; stop) stop ;; status) status $PROGNAME ;; restart) stop start ;; *) echo \"Usage: service luban_agent {start|stop|status|restart}\" exit 1 ;; esac EOF chmod a+x /etc/init.d/luban_agent service luban_agent start chkconfig luban_agent on centos73\u7684\u7269\u7406\u670d\u52a1\u5668 #!/bin/sh export http_server=\"http://10.177.9.11:8888/luban/\" export bin_dir=\"/opt/luban/bin\" mkdir -p $bin_dir rm -rf $bin_dir/* curl -o $bin_dir/luban_agent $http_server/luban_agent chmod a+x $bin_dir/* rm -rf /var/log/luban_agent.log cat > /opt/luban/bin/start_agent.sh << EOF nohup $bin_dir/luban_agent >/var/log/luban_agent.log 2>&1 & EOF chmod a+x /opt/luban/bin/start_agent.sh cat > /opt/luban/bin/stop_agent.sh << EOF ps -aux |grep luban_agent|grep -v grep|awk '{print $2}'|xargs kill -9 EOF chmod a+x /opt/luban/bin/stop_agent.sh cat > /usr/lib/systemd/system/luban_agent.service << EOF [Unit] Description=Luban agent After=network.target [Service] ExecStart=/opt/luban/bin/luban_agent > /var/log/luban_agent.log ExecStop=kill -9 `pidof luban_agent` Type=notify User=root Group=root [Install] WantedBy=multi-user.target EOF systemctl enable luban_agent systemctl start luban_agent","title":"\u5b89\u88c5\u9c81\u73edAgent"},{"location":"#node_exporter","text":"Node_exporter\u8d1f\u8d23\u76d1\u63a7\u7269\u7406\u673a\u670d\u52a1\u5668\u4ee5\u53ca\u90e8\u7f72\u94f6\u6cb3\u670d\u52a1\u7684\u865a\u62df\u673a\u76d1\u63a7\u4fe1\u606f\uff0c\u76d1\u63a7\u4e3b\u8981\u5305\u62eccpu\uff0c\u5185\u5b58\uff0c\u78c1\u76d8\uff0c\u7f51\u7edc \u5b89\u88c5\u6b65\u9aa4 #!/bin/sh export node_exporter_url=\"http://10.177.152.168:8888/luban/luban_on_k8s/install/node_exporter/node_exporter\" mkdir -p /opt/luban/bin/ export node_exporter_binary=\"/opt/luban/bin/node_exporter\" rm -rf $node_exporter_binary curl -o $node_exporter_binary ${node_exporter_url} chmod a+x $node_exporter_binary ps -aux |grep $node_exporter_binary|awk '{print $2}'|xargs kill -9 nohup $node_exporter_binary --collector.luban >/var/log/node_exporter.log 2>&1 &","title":"\u5b89\u88c5Node_exporter"},{"location":"#process_exporter","text":"Process_exporter\u8d1f\u8d23\u76d1\u63a7\u7269\u7406\u673a\u670d\u52a1\u5668\u4ee5\u53ca\u90e8\u7f72\u94f6\u6cb3\u670d\u52a1\u7684\u865a\u62df\u673a\u76d1\u63a7\u4fe1\u606f\uff0c\u76d1\u63a7\u4e3b\u8981\u5305\u62ec\u8fdb\u7a0b\u76d1\u63a7 \u5b89\u88c5\u6b65\u9aa4 #!/bin/sh export process_exporter_url=\"http://10.177.152.168:8888/luban/luban_on_k8s/install/process_exporter/process-exporter\" mkdir -p /opt/luban/bin/ mkdir -p /opt/luban/conf/ export process_exporter_binary=\"/opt/luban/bin/process-exporter\" rm -rf $process_exporter_binary curl -o $process_exporter_binary ${process_exporter_url} chmod a+x $process_exporter_binary ps -aux |grep $process_exporter_binary |grep -v grep|awk '{print $2}'|xargs kill -9 if [ ! -f \"/opt/luban/conf/process-name.yaml\" ];then echo \"/opt/luban/conf/process-name.yaml not exist\" else cp -rf /opt/luban/conf/process-name.yaml /opt/luban/conf/process-name.yaml.bak fi cat > /opt/luban/conf/process-name.yaml <<EOF process_names: - name: \"{ {.Comm}}\" cmdline: - '.+' EOF nohup $process_exporter_binary -config.path=/opt/luban/conf/process-name.yaml >/var/log/process_exporter.log 2>&1 &","title":"\u5b89\u88c5Process_exporter"},{"location":"#blackbox_exporter","text":"Blackbox_exporter\u8d1f\u8d23\u76d1\u63a7Ping\uff0cDNS\uff0cTelnet\u7b49\u76d1\u63a7 \u5b89\u88c5\u6b65\u9aa4 #!/bin/sh kubectl apply -f blackbox_alert.yaml -f blackbox_exporter.yaml -f blackbox_monitor.yaml -n monitoring","title":"\u5b89\u88c5Blackbox_exporter"},{"location":"#cluster_exporter","text":"Cluster_exporter\u8d1f\u8d23\u76d1\u63a7Ebs,Ks3\u7b49\u5b58\u50a8\u603b\u91cf\u4fe1\u606f\u4ee5\u53ca\u4f7f\u7528\u91cf\u4fe1\u606f\u7b49\u76d1\u63a7 \u5b89\u88c5\u6b65\u9aa4 #!/bin/sh kubectl apply -f cluster_exporter.yaml -n monitoring ## \u5b89\u88c5agent,\u4ee5\u4e0b10.177.9.1 \u7269\u7406\u670d\u52a1\u5668\u4e3a\u4f8b\uff0cssh\u767b\u5f55\u670d\u52a1\u5668,10.177.9.11\u4e3a\u9c81\u73ed\u7684http\u6587\u4ef6\u670d\u52a1\u5668 export http_server=\"http://10.177.9.11:8888/luban/\" export bin_dir=\"/opt/luban/bin\" mkdir -p $bin_dir rm -rf $bin_dir/* curl -o $bin_dir/node_exporter http://10.177.9.11:8888/luban/node_exporter curl -o $bin_dir/luban_agent http://10.177.9.11:8888/luban/luban_agent chmod a+x $bin_dir/* ## restart node_exporter ps -aux |grep node_exporter|grep -v grep|awk '{print $2}'|xargs kill -9 echo \"node_exporter shutdown [ok]\" ## restart luban_agent ps -aux |grep luban_agent|grep -v grep|awk '{print $2}'|xargs kill -9 echo \"luban_agent shutdown [ok]\" ## start node_exporter and luban_agent nohup $bin_dir/node_exporter --collector.luban >/var/log/node_exporter.log 2>&1 & echo \"node_exporter start [ok]\" nohup $bin_dir/luban_agent >/var/log/luban_agent.log 2>&1 & echo \"luban_agent start [ok]\" \u5b89\u88c5\u5b8c\u6210\u540e\u68c0\u67e5 node_exporter \u4ee5\u53caluban_agent \u662f\u5426\u6b63\u786e\u8fd0\u884c ps aux |grrep node_exporter ps aux |grrep luban_agent","title":"\u5b89\u88c5Cluster_exporter"},{"location":"#_10","text":"\u6b64\u6b65\u9aa4\u7531\u8fd0\u7ef4\u540c\u5b66\u5b89\u88c5\u5e95\u5c42\u94f6\u6cb3\u73af\u5883\u65f6\u5019\uff0c\u81ea\u52a8\u5316\u5b89\u88c5\uff0c\u81ea\u5b9a\u4e49\u76d1\u63a7\u811a\u672c\u5b58\u653e\u4f4d\u7f6e http://newgit.op.ksyun.com/galaxy_cloud/monitor_scripts/tree/master/sdn_monitor_shell\uff0c\u7531\u8fd0\u7ef4\u540c\u5b66\u81ea\u52a8\u5316\u5b89\u88c5\uff0c\u4ee5\u4e0b\u793a\u4f8b\u4f9b\u7814\u53d1\u540c\u5b66\u53c2\u8003 \u4ee5\u4e0bmem_monitor.sh\u4e3a\u4f8b\uff0c\u811a\u672c\u5185\u5bb9\u5982\u4e0b\uff1a #!/bin/bash # \u5185\u5b58\u76d1\u63a7\u811a\u672c source /etc/profile function send_data(){ ts=`date +%s`; metric=$1; endpoint=$4; value=$2; tags=$3 curl -X POST -d \"[{\\\"metric\\\": \\\"$metric\\\", \\\"endpoint\\\": \\\"$endpoint\\\", \\\"timestamp\\\": $ts,\\\"step\\\": 60,\\\"value\\\": $value,\\\"counterType\\\": \\\"GAUGE\\\",\\\"tags\\\": \\\"$tags\\\"}]\" http://127.0.0.1:1988/v1/push } item='mem_check' item1='crash_check' day=`date +%d` tag1='metric=crash_status' tag2='metric=/var/log/mcelog' tag3='metric=/var/log/messages' tag4='metric=/var/log/dmesg' now_time=`date +%F` hostn=`hostname` # \u5224\u65ad\u662f\u5426\u6709crash\u65e5\u5fd7 function crashlog(){ crash_log=`ls /var/crash/|grep \"$now_time\" |wc -l` if [ $crash_log -eq 0 ];then crash_status=1 else crash_status=0 fi echo $item $crash_status $tag1 $hostn crash send_data $item1 $crash_status $tag1 $hostn crash } # \u5224\u65ad\u662f\u5426\u6709\u5185\u5b58\u65e5\u5fd7\u6587\u4ef6 function mcelog(){ test -a /var/log/mcelog if [ $? -eq 0 ];then counts=`cat /var/log/mcelog|wc -l` if [ $counts -eq 0 ];then mcelog_status=1 else mcelog_status=0 fi else mcelog_status=1 fi echo $item $mcelog_status $tag2 $hostn mcelog send_data $item $mcelog_status $tag2 $hostn mcelog } # \u5224\u65ad\u65e5\u5fd7 function messagelog (){ message_log=`tail -n500 /var/log/messages|egrep -i 'err|fail' |egrep -i \"memory\" |egrep -v 'nova.compute.manager'|wc -l` if [ $message_log -gt 1 ];then mem_status=0 else mem_status=1 fi echo $item $mem_status $tag3 $hostn message send_data $item $mem_status $tag3 $hostn message } # \u5224\u65ad\u65e5\u5fd7 function dmesglog (){ dme_log1=`tail -n500 /var/log/dmesg|egrep -i \"err|fail\" |egrep -i \"memory\" |wc -l` #dme_log2=`ssh $i dmesg |tail -n200 |egrep -i 'error|fail'|wc -l` if [ $dme_log1 -gt 1 ];then dme_status=0 else dme_status=1 fi echo $item $dme_status $tag4 $hostn dmesg send_data $item $dme_status $tag4 $hostn dmesg } crashlog mcelog messagelog dmesglog \u914d\u7f6e\u7269\u7406\u670d\u52a1\u5668\u4e0a\u7684crontab\uff0c\u5982\u4e0b\u56fe\u6240\u793a cat /etc/crontab SHELL=/bin/bash PATH=/sbin:/bin:/usr/sbin:/usr/bin MAILTO=root HOME=/ # For details see man 4 crontabs # Example of job definition: # .---------------- minute (0 - 59) # | .------------- hour (0 - 23) # | | .---------- day of month (1 - 31) # | | | .------- month (1 - 12) OR jan,feb,mar,apr ... # | | | | .---- day of week (0 - 6) (Sunday=0 or 7) OR sun,mon,tue,wed,thu,fri,sat # | | | | | # * * * * * user-name command to be executed */1 * * * * root bash /opt/luban/script/mem_monitor.sh","title":"\u5b89\u88c5\u81ea\u5b9a\u4e49\u76d1\u63a7\u811a\u672c"},{"location":"#_11","text":"\u76ee\u524d\u65e5\u5fd7\u7684\u7ec4\u4ef6\u91c7\u7528filebeat+kafka+logstash+ELK\u67b6\u6784,\u53c2\u7167http://ezone.ksyun.com/ezCode/luban/luban_on_k8s/tree\u4e2d\u7684logging\u6587\u4ef6\u5939\u4e2d\u7684README.md \u9700\u8981\u79bb\u7ebf\u955c\u50cf\u5217\u8868 harbor.inner.galaxy.ksyun.com/luban/elasticsearch:7.10.1 harbor.inner.galaxy.ksyun.com/luban/kibana:7.10.1 harbor.inner.galaxy.ksyun.com/luban/filebeat:7.10.1 harbor.inner.galaxy.ksyun.com/luban/kafka:2.7.0-debian-10-r1 harbor.inner.galaxy.ksyun.com/luban/zookeeper:3.6.2-debian-10-r89 harbor.inner.galaxy.ksyun.com/luban/logstash:7.10.1","title":"\u5b89\u88c5\u65e5\u5fd7\u7ec4\u4ef6"},{"location":"#elasticsearch","text":"kubectl create ns elastic-system export REGISTRY=\"harbor.inner.galaxy.ksyun.com\" helm install es ./elasticsearch-7.10.1.tgz -n elastic-system --set imageTag=7.10.1 --set persistence.enabled=false --set image=$REGISTRY/luban/elasticsearch","title":"\u5b89\u88c5Elasticsearch"},{"location":"#kibana","text":"kubectl create ns elastic-system export REGISTRY=\"harbor.inner.galaxy.ksyun.com\" helm install kibana -n elastic-system ./kibana-7.10.1.tgz --set imageTag=7.10.1 --set image=$REGISTRY/luban/kibana","title":"\u5b89\u88c5Kibana"},{"location":"#kafka","text":"\u6ce8\u610fkafka\u96c6\u7fa4\u9700\u8981\u96c6\u7fa4\u5916\u8bbf\u95ee\u6743\u9650\uff0c\u6700\u597d\u589e\u52a0eip\u7684slb\u7ed1\u5b9akubernetes\u96c6\u7fa4\u7684\u4e09\u4e2amaster\u8282\u70b9 kubectl create ns elastic-system export REGISTRY=\"harbor.inner.galaxy.ksyun.com\" helm install kafka ./kafka-12.4.2.tgz -n elastic-system --set persistence.enabled=false --set image.registry=$REGISTRY --set image.repository=luban/kafka --set image.tag=2.7.0-debian-10-r1 --set zookeeper.image.registry=$REGISTRY --set zookeeper.image.repository=luban/zookeeper --set zookeeper.image.tag=3.6.2-debian-10-r89 --set zookeeper.persistence.enabled=false --set replicaCount=3 \\ --set externalAccess.enabled=true \\ --set externalAccess.service.type=NodePort \\ --set externalAccess.autoDiscovery.enabled=false \\ --set serviceAccount.create=true \\ --set rbac.create=true \\ --set externalAccess.service.nodePorts='{30092,30093,30094}' \\ --set externalAccess.service.domain=10.177.152.168 ##3*master\u8282\u70b9\u7684\u7684slb\u7684eip","title":"\u5b89\u88c5Kafka"},{"location":"#filebeates","text":"kubectl create ns elastic-system export REGISTRY=\"harbor.inner.galaxy.ksyun.com\" helm install filebeat ./filebeat-7.10.1.tgz -n elastic-system --set imageTag=7.10.1 --set image=$REGISTRY/luban/filebeat","title":"\u5b89\u88c5Filebeat\uff08\u96c6\u7fa4\u5185\u90e8\uff0c\u7528\u4e8e\u6536\u96c6\u5bb9\u5668\u65e5\u5fd7\uff0c\u76f4\u63a5\u4e0a\u4f20es\u96c6\u7fa4\uff09"},{"location":"#filebeatkafka","text":"rpm -ivh filebeat-7.10.1-x86_64.rpm \u4fee\u6539/etc/filebeat/filebeat.yml\u914d\u7f6e\uff0c\u5b98\u65b9\u5730\u5740\uff1ahttps://www.elastic.co/guide/en/beats/filebeat/current/configuring-howto-filebeat.html","title":"\u5b89\u88c5Filebeat\uff08\u96c6\u7fa4\u5916\u90e8\uff0c\u7528\u4e8e\u6536\u96c6\u670d\u52a1\u5668\u6216\u8005\u5176\u4ed6\u7269\u7406\u8bbe\u5907\u65e5\u5fd7\uff0c\u5199\u5165kafka\uff09"},{"location":"#logstash","text":"\u6ce8\u610f\u4fee\u6539logstash\u7684\u914d\u7f6e\u6587\u4ef6values.yaml kubectl create ns elastic-system export REGISTRY=\"harbor.inner.galaxy.ksyun.com\" helm install logstash ./logstash -n elastic-system --set imageTag=7.10.1 --set image=$REGISTRY/luban/logstash","title":"\u5b89\u88c5Logstash"},{"location":"#curator","text":"helm install curator ./elasticsearch-curator -n elastic-system --set image.repository=harbor.inner.galaxy.ksyun.com/luban/curator --set image.tag=5.7.6 \u4ee5\u4e0b\u662f\u81ea\u52a8\u5316\u811a\u672c kubectl create ns elastic-system export REGISTRY=\"harbor.inner.galaxy.ksyun.com\" helm install es ./elasticsearch-7.10.1.tgz -n elastic-system --set imageTag=7.10.1 --set persistence.enabled=false --set image=$REGISTRY/luban/elasticsearch helm install kibana -n elastic-system ./kibana-7.10.1.tgz --set imageTag=7.10.1 --set image=$REGISTRY/luban/kibana helm install filebeat ./filebeat-7.10.1.tgz -n elastic-system --set imageTag=7.10.1 --set image=$REGISTRY/luban/filebeat ## \u6ce8\u610f\u4fee\u6539kafka\u7684externalAccess.service.domain\uff0c\u7528\u4e8e\u5176\u4ed6\u7269\u7406\u670d\u52a1\u5668filebeat\u914d\u7f6e\uff0c10.177.152.168\u5e94\u4e3aK8s\u7684Master\u8282\u70b9\u8d1f\u8f7d\u5747\u8861IP helm install kafka ./kafka-12.4.2.tgz -n elastic-system --set persistence.enabled=false --set image.registry=$REGISTRY --set image.repository=luban/kafka --set image.tag=2.7.0-debian-10-r1 --set zookeeper.image.registry=$REGISTRY --set zookeeper.image.repository=luban/zookeeper --set zookeeper.image.tag=3.6.2-debian-10-r89 --set zookeeper.persistence.enabled=false --set replicaCount=3 \\ --set externalAccess.enabled=true \\ --set externalAccess.service.type=NodePort \\ --set externalAccess.autoDiscovery.enabled=false \\ --set serviceAccount.create=true \\ --set rbac.create=true \\ --set externalAccess.service.nodePorts='{30092,30093,30094}' \\ --set externalAccess.service.domain=10.177.152.168 helm install logstash ./logstash -n elastic-system --set imageTag=7.10.1 --set image=$REGISTRY/luban/logstash helm install curator ./elasticsearch-curator -n elastic-system --set image.repository=$REGISTRY/luban/curator --set image.tag=5.7.6","title":"\u5b89\u88c5\u81ea\u52a8\u6e05\u7406Curator"},{"location":"#cmdb_1","text":"\u53c2\u7167aws\u7684dgraph HA\u67b6\u6784\uff0c\u539f\u6587\u5730\u5740\uff1a https://aws.amazon.com/cn/blogs/opensource/dgraph-on-aws-setting-up-a-horizontally-scalable-graph-database/ MANIFEST=\"https://raw.githubusercontent.com/dgraph-io/dgraph/master/contrib/config/kubernetes/dgraph-ha/dgraph-ha.yaml\" kubectl apply --filename $MANIFEST","title":"\u5b89\u88c5Cmdb\u7ec4\u4ef6"},{"location":"#_12","text":"","title":"\u5b89\u88c5\u8ba4\u8bc1\u670d\u52a1"},{"location":"#dex","text":"\u5f00\u6e90\u9879\u76eedex\uff0c\u4e00\u4e2a\u57fa\u4e8eOpenID Connect\u7684\u8eab\u4efd\u670d\u52a1\u7ec4\u4ef6\u3002 CoreOS\u5df2\u7ecf\u5c06\u5b83\u7528\u4e8e\u751f\u4ea7\u73af\u5883\uff0c\u7528\u6237\u8ba4\u8bc1\u548c\u6388\u6743\u662f\u5e94\u7528\u5b89\u5168\u7684\u4e00\u4e2a\u91cd\u8981\u90e8\u5206\uff0c\u7528\u6237\u8eab\u4efd\u7ba1\u7406\u672c\u8eab\u4e5f\u662f\u4e00\u4e2a\u7279\u522b\u4e13\u4e1a\u548c\u590d\u6742\u7684\u95ee\u9898\uff0c\u5c24\u5176\u5bf9\u4e8e\u4f01\u4e1a\u5e94\u7528\u800c\u8a00\uff0c \u5b89\u5168\u7684\u8fdb\u884c\u8ba4\u8bc1\u548c\u6388\u6743\u662f\u5fc5\u9009\u9879\uff0cdex\u65e0\u7591\u662f\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u7684\u4e00\u5927\u5229\u5668\u3002 ```shell kubectl create ns dex kubectl -n dex delete secret dex.example.com.tls kubectl -n dex create secret tls dex.example.com.tls --cert=./ssl/cert.pem --key=./ssl/key.pem cat <<EOF | kubectl -n dex apply -f - apiVersion: apps/v1 kind: Deployment metadata: labels: app: dex name: dex namespace: dex spec: replicas: 1 selector: matchLabels: app: dex template: metadata: labels: app: dex spec: serviceAccountName: dex # This is created below containers: - image: harbor.inner.galaxy.ksyun.com/luban/dex:v2.27.0 #or quay.io/dexidp/dex:v2.26.0 name: dex command: [\"/usr/local/bin/dex\", \"serve\", \"/etc/dex/cfg/config.yaml\"] ports: - name: https containerPort: 5556 volumeMounts: - name: config mountPath: /etc/dex/cfg - name: tls mountPath: /etc/dex/tls - name: data mountPath: /data volumes: - name: config configMap: name: dex items: - key: config.yaml path: config.yaml - name: tls secret: secretName: dex.example.com.tls - name: data emptyDir: {} apiVersion: v1 kind: ConfigMap metadata: name: dex namespace: dex data: config.yaml: | issuer: https://luban.dex.galaxy.cloud/dex storage: type: memory logger: level: debug format: text web: http: 0.0.0.0:5556 expiry: signingKeys: \"6h\" idTokens: \"24h\" connectors: - type: ldap name: ActiveDirectory id: ladp config: host: openldap:389 insecureNoSSL: true insecureSkipVerify: true bindDN: cn=admin,dc=galaxy,dc=cloud bindPW: Kingsoft123 usernamePrompt: Email Address userSearch: baseDN: ou=People,dc=galaxy,dc=cloud filter: \"(objectClass=inetOrgPerson)\" username: mail idAttr: uid emailAttr: mail nameAttr: uid groupSearch: baseDN: ou=Groups,dc=galaxy,dc=cloud filter: \"(objectClass=groupOfUniqueNames)\" userAttr: uid groupAttr: memberUid nameAttr: cn oauth2: skipApprovalScreen: true logger: level: \"debug\" format: text staticClients: - id: oidc-auth-client redirectURIs: - 'https://luban.auth.galaxy.cloud/callback' - 'https://luban.idp.galaxy.cloud/idp/v1/token/callback' - 'http://localhost/idp/v1/token/callback' name: 'oidc-auth-client' secret: ZXhhbXBsZS1hcHAtc2VjcmV0 apiVersion: v1 kind: Service metadata: name: dex namespace: dex spec: ports: - name: dex port: 5556 protocol: TCP targetPort: 5556 selector: app: dex apiVersion: v1 kind: ServiceAccount metadata: labels: app: dex name: dex namespace: dex apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: dex rules: - apiGroups: [\"dex.coreos.com\"] # API group created by dex resources: [\" \"] verbs: [\" \"] - apiGroups: [\"apiextensions.k8s.io\"] resources: [\"customresourcedefinitions\"] verbs: [\"create\"] # To manage its own resources, dex must be able to create customresourcedefinitions apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: dex roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: dex subjects: - kind: ServiceAccount name: dex # Service account assigned to the dex pod, created above namespace: dex # The namespace dex is running in apiVersion: apiextensions.k8s.io/v1beta1 kind: CustomResourceDefinition metadata: name: authcodes.dex.coreos.com spec: group: dex.coreos.com names: kind: AuthCode listKind: AuthCodeList plural: authcodes singular: authcode scope: Namespaced version: v1 apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRole metadata: name: dex rules: - apiGroups: [\"dex.coreos.com\"] # API group created by dex resources: [\" \"] verbs: [\" \"] - apiGroups: [\"apiextensions.k8s.io\"] resources: [\"customresourcedefinitions\"] verbs: [\"create\"] # To manage its own resources identity must be able to create customresourcedefinitions. apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: dex roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: dex subjects: - kind: ServiceAccount name: dex # Service account assigned to the dex pod. namespace: dex EOF \u200b","title":"\u5b89\u88c5Dex"},{"location":"#ldap","text":"Open-ldap kubectl create ns dex helm install openldap ./openldap-1.2.7.tgz -f online-values.yaml -n dex ## get password $ kubectl -n dex get secret openldap -o jsonpath=\"{.data.LDAP_ADMIN_PASSWORD}\" | base64 --decode; echo $ kubectl -n dex get secret openldap -o jsonpath=\"{.data.LDAP_CONFIG_PASSWORD}\" | base64 --decode; echo # node add ldap label $ kubectl label nodes ip-10-178-224-65 luban/ldap=\"\" --overwrite ldapsearch -x -H ldap://openldap:389 \\ -b dc=galaxy,dc=cloud \\ -D \"cn=admin,dc=galaxy,dc=cloud\" \\ -w Kingsoft123","title":"\u5b89\u88c5Ldap"},{"location":"#kubernetes_1","text":"## set kubernetes apiserver - --oidc-issuer-url=https://luban.dex.galaxy.cloud/dex - --oidc-client-id=oidc-auth-client - --oidc-ca-file=/etc/kubernetes/ssl/dex-ca.pem - --oidc-username-claim=email - --oidc-groups-claim=groups","title":"\u914d\u7f6eKubernetes\u8ba4\u8bc1\u670d\u52a1"},{"location":"#_13","text":"","title":"\u5b89\u88c5\u9c81\u73ed\u670d\u52a1"},{"location":"#server","text":"--- apiVersion: apps/v1 kind: Deployment metadata: labels: app: server name: server namespace: luban spec: progressDeadlineSeconds: 600 replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: app: server strategy: rollingUpdate: maxSurge: 25% maxUnavailable: 25% type: RollingUpdate template: metadata: creationTimestamp: null labels: app: server spec: containers: - image: harbor.inner.galaxy.ksyun.com/luban/server imagePullPolicy: Always name: server resources: {} terminationMessagePath: /dev/termination-log terminationMessagePolicy: File dnsPolicy: ClusterFirst restartPolicy: Always schedulerName: default-scheduler securityContext: {} terminationGracePeriodSeconds: 30 --- apiVersion: v1 kind: Service metadata: labels: app: server name: server namespace: luban spec: ports: - port: 80 protocol: TCP targetPort: 80 selector: app: server sessionAffinity: None type: ClusterIP --- apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: server-gateway spec: selector: istio: ingressgateway # use Istio default gateway implementation servers: - port: number: 80 name: http protocol: HTTP hosts: - \"luban.server.galaxy.cloud\" --- apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: server spec: hosts: - \"luban.server.galaxy.cloud\" gateways: - server-gateway http: - match: - uri: prefix: / route: - destination: port: number: 80 host: server corsPolicy: allowOrigins: - exact: \"*\" allowMethods: - GET - POST - PATCH - PUT - DELETE - OPTIONS allowCredentials: false allowHeaders: - authorization maxAge: \"24h\"","title":"\u5b89\u88c5\u9c81\u73edServer"},{"location":"#idp","text":"--- apiVersion: apps/v1 kind: Deployment metadata: labels: app: idp name: idp namespace: luban spec: progressDeadlineSeconds: 600 replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: app: idp strategy: rollingUpdate: maxSurge: 25% maxUnavailable: 25% type: RollingUpdate template: metadata: creationTimestamp: null labels: app: idp spec: containers: - args: - --tls_cert= - --tls_key= - --domain=luban.idp.galaxy.cloud - --ldap_base=dc=galaxy,dc=cloud - --ldap_bind_password=Kingsoft123 - --ldap_bind_user=admin - --ldap_group_ou=Groups - --ldap_host=ldap-openldap.dex - --ldap_port=389 - --ldap_user_ou=People - --listin_addr=0.0.0.0 - --listin_port=80 - --oauth_client_id=oidc-auth-client - --oauth_client_secrect=ZXhhbXBsZS1hcHAtc2VjcmV0 - --oauth_redirect_url=http://localhost/idp/v1/token/callback - --oauth_scopes=openid,profile,email,offline_access,groups - --oauth_token_url=http://dex.dex:5556/dex/token - --oauth_url=http://dex.dex:5556/dex/auth command: - /app image: harbor.inner.galaxy.ksyun.com/luban/idp imagePullPolicy: Always name: server resources: {} terminationMessagePath: /dev/termination-log terminationMessagePolicy: File dnsPolicy: ClusterFirst restartPolicy: Always schedulerName: default-scheduler securityContext: {} terminationGracePeriodSeconds: 30 --- apiVersion: v1 kind: Service metadata: labels: app: idp name: idp namespace: luban spec: ports: - port: 80 protocol: TCP targetPort: 80 selector: app: idp sessionAffinity: None type: ClusterIP --- apiVersion: networking.istio.io/v1beta1 kind: VirtualService metadata: name: idp namespace: luban spec: gateways: - idp-gateway hosts: - luban.idp.galaxy.cloud http: - corsPolicy: allowCredentials: false allowHeaders: - authorization allowMethods: - GET - POST - PATCH - PUT - DELETE - OPTIONS allowOrigins: - exact: '*' maxAge: 24h route: - destination: host: idp --- apiVersion: networking.istio.io/v1beta1 kind: Gateway metadata: name: idp-gateway namespace: luban spec: selector: istio: ingressgateway servers: - hosts: - luban.idp.galaxy.cloud port: name: https number: 443 protocol: HTTPS tls: mode: PASSTHROUGH - hosts: - luban.idp.galaxy.cloud port: name: http number: 80 protocol: HTTP","title":"\u5b89\u88c5\u9c81\u73edIdp"},{"location":"#cmdbapi","text":"--- apiVersion: apps/v1 kind: Deployment metadata: labels: app: cmdb name: cmdb namespace: luban spec: progressDeadlineSeconds: 600 replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: app: cmdb strategy: rollingUpdate: maxSurge: 25% maxUnavailable: 25% type: RollingUpdate template: metadata: creationTimestamp: null labels: app: cmdb spec: containers: - env: - name: OS_USERNAME value: admin - name: OS_TENANT_NAME value: admin - name: OS_PASSWORD value: ksc - name: OS_AUTH_URL value: http://10.177.147.1:35357/v2.0 - name: OS_REGION_NAME value: SHPBSRegionOne image: harbor.inner.galaxy.ksyun.com/luban/cmdb-api imagePullPolicy: Always name: cmdb resources: {} terminationMessagePath: /dev/termination-log terminationMessagePolicy: File dnsPolicy: ClusterFirst restartPolicy: Always schedulerName: default-scheduler securityContext: {} terminationGracePeriodSeconds: 30 --- apiVersion: v1 kind: Service metadata: labels: app: cmdb name: cmdb namespace: luban spec: ports: - port: 80 protocol: TCP targetPort: 80 selector: app: cmdb sessionAffinity: None type: ClusterIP --- apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: cmdb-gateway spec: selector: istio: ingressgateway # use Istio default gateway implementation servers: - port: number: 80 name: http protocol: HTTP hosts: - \"luban.cmdb.galaxy.cloud\" --- apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: cmdb spec: hosts: - \"luban.cmdb.galaxy.cloud\" gateways: - cmdb-gateway http: - match: - uri: prefix: / route: - destination: port: number: 80 host: cmdb corsPolicy: allowOrigins: - exact: \"*\" allowMethods: - GET - POST - PATCH - PUT - DELETE - OPTIONS allowCredentials: false allowHeaders: - authorization maxAge: \"24h\"","title":"\u5b89\u88c5\u9c81\u73edCmdbApi"},{"location":"#swagger","text":"--- apiVersion: apps/v1 kind: Deployment metadata: labels: app: swagger-ui name: swagger-ui namespace: luban spec: progressDeadlineSeconds: 600 replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: app: swagger-ui strategy: rollingUpdate: maxSurge: 25% maxUnavailable: 25% type: RollingUpdate template: metadata: creationTimestamp: null labels: app: swagger-ui spec: containers: - image: harbor.inner.galaxy.ksyun.com/luban/swagger-ui imagePullPolicy: Always name: swagger-ui resources: {} terminationMessagePath: /dev/termination-log terminationMessagePolicy: File dnsPolicy: ClusterFirst restartPolicy: Always schedulerName: default-scheduler securityContext: {} terminationGracePeriodSeconds: 30 --- apiVersion: v1 kind: Service metadata: labels: app: swagger-ui name: swagger-ui namespace: luban spec: ports: - port: 8080 protocol: TCP targetPort: 8080 selector: app: swagger-ui sessionAffinity: None type: ClusterIP --- apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: swagger-gateway spec: selector: istio: ingressgateway # use Istio default gateway implementation servers: - port: number: 80 name: http protocol: HTTP hosts: - \"luban.swagger.galaxy.cloud\" --- apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: swagger spec: hosts: - \"luban.swagger.galaxy.cloud\" gateways: - swagger-gateway http: - match: - uri: prefix: / route: - destination: port: number: 8080 host: swagger-ui #!/bin/sh kubectl apply -f cmdb-api.yaml -f luban-idp.yaml -f luban-server.yaml -f swagger.yaml -n luban","title":"\u5b89\u88c5\u9c81\u73edSwagger"},{"location":"#elasticsearch_1","text":"apiVersion: apps/v1 kind: Deployment metadata: labels: app: alertmanager-warning name: alertmanager-warning namespace: monitoring spec: progressDeadlineSeconds: 600 replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: app: alertmanager-warning strategy: rollingUpdate: maxSurge: 25% maxUnavailable: 25% type: RollingUpdate template: metadata: creationTimestamp: null labels: app: alertmanager-warning spec: containers: - image: harbor.inner.galaxy.ksyun.com/luban/alertmanager-output imagePullPolicy: Always name: alertmanager-output resources: {} terminationMessagePath: /dev/termination-log terminationMessagePolicy: File dnsPolicy: ClusterFirst restartPolicy: Always schedulerName: default-scheduler securityContext: {} terminationGracePeriodSeconds: 30 --- apiVersion: v1 kind: Service metadata: labels: app: alertmanager-warning name: alertmanager-warning namespace: monitoring spec: ports: - port: 8080 protocol: TCP targetPort: 8080 selector: app: alertmanager-warning sessionAffinity: None type: ClusterIP --- apiVersion: monitoring.coreos.com/v1alpha1 kind: AlertmanagerConfig metadata: labels: alertmanagerConfig: warning release: pm name: warning namespace: monitoring spec: receivers: - name: warning-hook webhookConfigs: - url: http://alertmanager-warning:8080/webhook route: groupBy: - alertname groupInterval: 1m groupWait: 30s receiver: warning-hook repeatInterval: 1m","title":"\u5b89\u88c5\u544a\u8b66\u5bf9\u63a5ElasticSearch"},{"location":"#_14","text":"apiVersion: apps/v1 kind: Deployment metadata: labels: app: alertmanager-feishu name: alertmanager-feishu namespace: monitoring spec: progressDeadlineSeconds: 600 replicas: 0 revisionHistoryLimit: 10 selector: matchLabels: app: alertmanager-feishu strategy: rollingUpdate: maxSurge: 25% maxUnavailable: 25% type: RollingUpdate template: metadata: creationTimestamp: null labels: app: alertmanager-feishu spec: containers: - image: harbor.inner.galaxy.ksyun.com/luban/feishu-webhook:latest imagePullPolicy: Always name: feishu-webhook resources: {} terminationMessagePath: /dev/termination-log terminationMessagePolicy: File dnsPolicy: ClusterFirst restartPolicy: Always schedulerName: default-scheduler securityContext: {} terminationGracePeriodSeconds: 30 --- apiVersion: v1 kind: Service metadata: labels: app: alertmanager-feishu name: alertmanager-feishu namespace: monitoring spec: ports: - port: 8080 protocol: TCP targetPort: 8080 selector: app: alertmanager-feishu sessionAffinity: None type: ClusterIP --- apiVersion: monitoring.coreos.com/v1alpha1 kind: AlertmanagerConfig metadata: labels: alertmanagerConfig: feshu release: pm name: feishu namespace: monitoring spec: receivers: - name: feishu-hook webhookConfigs: - url: http://alertmanager-feishu:8080/alertmanager-alert route: groupBy: - alertname groupInterval: 1m groupWait: 30s receiver: feishu-hook repeatInterval: 1m","title":"\u5b89\u88c5\u544a\u8b66\u5bf9\u63a5\u98de\u4e66"},{"location":"#_15","text":"kubectl apply -f prometheus-rules -n monitoring","title":"\u5bfc\u5165\u94f6\u6cb3\u544a\u8b66\u89c4\u5219"},{"location":"#_16","text":"\u76ee\u524d\u9636\u6bb5\u9700\u8981\u624b\u52a8\u5b89\u88c5prometheus\u76d1\u63a7\u7684\u7269\u7406\u673a\u5217\u8868\uff0c\u540e\u7eed\u4f1a\u81ea\u52a8\u53d1\u73b0\u7269\u7406\u673a\u4ee5\u53ca\u90e8\u7f72\u94f6\u6cb3\u670d\u52a1\u7684\u865a\u62df\u673a\u5217\u8868\uff08\u6682\u65f6\u624b\u52a8\u5b89\u88c5\uff09 apiVersion: v1 kind: Service metadata: labels: k8s-app: external-nodes name: external-nodes namespace: monitoring spec: ports: - name: metrics port: 9100 protocol: TCP targetPort: 9100 sessionAffinity: None type: ClusterIP --- apiVersion: v1 kind: Endpoints metadata: labels: k8s-app: external-nodes name: external-nodes namespace: monitoring subsets: - addresses: - ip: 10.177.16.2 - ip: 10.177.16.3 - ip: 10.177.16.4 - ip: 10.177.16.5 - ip: 10.177.16.6 - ip: 10.177.16.7 - ip: 10.177.9.11 - ip: 10.178.225.17 ports: - name: metrics port: 9100 protocol: TCP --- apiVersion: monitoring.coreos.com/v1 kind: ServiceMonitor metadata: labels: k8s-app: external-nodes release: pm name: external-nodes namespace: monitoring spec: endpoints: - interval: 60s port: metrics namespaceSelector: matchNames: - monitoring selector: matchLabels: k8s-app: external-nodes","title":"\u5b89\u88c5\u76d1\u63a7\u7269\u7406\u673a"},{"location":"#_17","text":"","title":"\u5b89\u88c5\u9c81\u73ed\u63a7\u5236\u53f0"},{"location":"#base","text":"--- apiVersion: apps/v1 kind: Deployment metadata: labels: app: console-base name: console-base namespace: luban-fe spec: progressDeadlineSeconds: 600 replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: app: console-base strategy: rollingUpdate: maxSurge: 25% maxUnavailable: 25% type: RollingUpdate template: metadata: creationTimestamp: null labels: app: console-base spec: containers: - image: harbor.inner.galaxy.ksyun.com/watt/luban-fe/console-base:latest imagePullPolicy: Always name: console-base resources: {} terminationMessagePath: /dev/termination-log terminationMessagePolicy: File dnsPolicy: ClusterFirst restartPolicy: Always schedulerName: default-scheduler securityContext: {} terminationGracePeriodSeconds: 30 --- apiVersion: v1 kind: Service metadata: labels: app: console-base name: console-base namespace: luban-fe spec: ports: - port: 80 protocol: TCP targetPort: 80 selector: app: console-base sessionAffinity: None --- apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: console-gateway spec: selector: istio: ingressgateway # use Istio default gateway implementation servers: - port: number: 80 name: http protocol: HTTP hosts: - \"luban.console.galaxy.cloud\" --- apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: console spec: hosts: - \"luban.console.galaxy.cloud\" gateways: - console-gateway http: - match: - uri: prefix: / route: - destination: port: number: 80 host: console-base corsPolicy: allowOrigins: - exact: \"*\" allowMethods: - GET - POST - PATCH - PUT - DELETE - OPTIONS allowCredentials: false allowHeaders: - authorization maxAge: \"24h\"","title":"\u5b89\u88c5\u9c81\u73edBase"},{"location":"#system","text":"apiVersion: apps/v1 kind: Deployment metadata: labels: app: console-system name: console-system namespace: luban-fe spec: progressDeadlineSeconds: 600 replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: app: console-system strategy: rollingUpdate: maxSurge: 25% maxUnavailable: 25% type: RollingUpdate template: metadata: creationTimestamp: null labels: app: console-system spec: containers: - image: harbor.inner.galaxy.ksyun.com/watt/luban-fe/console-system:latest imagePullPolicy: Always name: console-system resources: {} terminationMessagePath: /dev/termination-log terminationMessagePolicy: File dnsPolicy: ClusterFirst restartPolicy: Always schedulerName: default-scheduler securityContext: {} terminationGracePeriodSeconds: 30 --- apiVersion: v1 kind: Service metadata: labels: app: console-system name: console-system namespace: luban-fe spec: ports: - port: 80 protocol: TCP targetPort: 80 selector: app: console-system sessionAffinity: None --- apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: system-gateway spec: selector: istio: ingressgateway # use Istio default gateway implementation servers: - port: number: 80 name: http protocol: HTTP hosts: - \"luban.system.galaxy.cloud\" --- apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: system spec: hosts: - \"luban.system.galaxy.cloud\" gateways: - system-gateway http: - match: - uri: prefix: / route: - destination: port: number: 80 host: console-system corsPolicy: allowOrigins: - exact: \"*\" allowMethods: - GET - POST - PATCH - PUT - DELETE - OPTIONS allowCredentials: false allowHeaders: - authorization maxAge: \"24h\"","title":"\u5b89\u88c5\u9c81\u73edSystem"},{"location":"#monitor","text":"--- apiVersion: apps/v1 kind: Deployment metadata: labels: app: console-monitor name: console-monitor namespace: luban-fe spec: progressDeadlineSeconds: 600 replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: app: console-monitor strategy: rollingUpdate: maxSurge: 25% maxUnavailable: 25% type: RollingUpdate template: metadata: creationTimestamp: null labels: app: console-monitor spec: containers: - image: harbor.inner.galaxy.ksyun.com/watt/luban-fe/console-monitor:latest imagePullPolicy: Always name: console-monitor resources: {} terminationMessagePath: /dev/termination-log terminationMessagePolicy: File dnsPolicy: ClusterFirst restartPolicy: Always schedulerName: default-scheduler securityContext: {} terminationGracePeriodSeconds: 30 --- apiVersion: v1 kind: Service metadata: labels: app: console-monitor name: console-monitor namespace: luban-fe spec: ports: - port: 80 protocol: TCP targetPort: 80 selector: app: console-monitor sessionAffinity: None --- apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: monitor-gateway spec: selector: istio: ingressgateway # use Istio default gateway implementation servers: - port: number: 80 name: http protocol: HTTP hosts: - \"luban.monitor.galaxy.cloud\" --- apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: monitor spec: hosts: - \"luban.monitor.galaxy.cloud\" gateways: - monitor-gateway http: - match: - uri: prefix: / route: - destination: port: number: 80 host: console-monitor corsPolicy: allowOrigins: - exact: \"*\" allowMethods: - GET - POST - PATCH - PUT - DELETE - OPTIONS allowCredentials: false allowHeaders: - authorization maxAge: \"24h\"","title":"\u5b89\u88c5\u9c81\u73edMonitor"},{"location":"#demo","text":"--- apiVersion: apps/v1 kind: Deployment metadata: labels: app: console-demo name: console-demo namespace: luban-fe spec: progressDeadlineSeconds: 600 replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: app: console-demo strategy: rollingUpdate: maxSurge: 25% maxUnavailable: 25% type: RollingUpdate template: metadata: creationTimestamp: null labels: app: console-demo spec: containers: - image: harbor.inner.galaxy.ksyun.com/watt/luban-fe/console-demo:latest imagePullPolicy: Always name: console-demo resources: {} terminationMessagePath: /dev/termination-log terminationMessagePolicy: File dnsPolicy: ClusterFirst restartPolicy: Always schedulerName: default-scheduler securityContext: {} terminationGracePeriodSeconds: 30 --- apiVersion: v1 kind: Service metadata: labels: app: console-demo name: console-demo namespace: luban-fe spec: ports: - port: 80 protocol: TCP targetPort: 80 selector: app: console-demo sessionAffinity: None --- apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: demo-gateway spec: selector: istio: ingressgateway # use Istio default gateway implementation servers: - port: number: 80 name: http protocol: HTTP hosts: - \"luban.demo.galaxy.cloud\" --- apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: demo spec: hosts: - \"luban.demo.galaxy.cloud\" gateways: - demo-gateway http: - match: - uri: prefix: / route: - destination: port: number: 80 host: console-demo corsPolicy: allowOrigins: - exact: \"*\" allowMethods: - GET - POST - PATCH - PUT - DELETE - OPTIONS allowCredentials: false allowHeaders: - authorization maxAge: \"24h\" \u5b89\u88c5\u547d\u4ee4 #!/bin/sh kubectl apply -f base.yaml -f demo.yaml -f monitor.yaml -f system.yaml -n luban-fe","title":"\u5b89\u88c5\u9c81\u73edDemo"},{"location":"#_18","text":"","title":"\u64cd\u4f5c\u624b\u518c"},{"location":"#_19","text":"","title":"\u76d1\u63a7"},{"location":"#prometheus","text":"\u76ee\u524d\u7814\u53d1\u9636\u6bb5\uff0c\u9700\u8981\u624b\u52a8\u914d\u7f6eprometheus\u91c7\u96c6\uff0c\u521d\u6b21\u521b\u5efa\u7684\u65f6\u5019\uff0c\u9700\u8981\u5728k8s\u4e2d\u521b\u5efaServiceMonitor\u7684crd\u8d44\u6e90\uff0c\u540e\u7eed\u4f1a\u914d\u5408cmdb\u505a\u670d\u52a1\u81ea\u52a8\u53d1\u73b0\uff0c\u6ce8\u518c\u5230k8s\u7684endpoint\u3002 \u9996\u6b21\u914d\u7f6e\u91c7\u96c6\u6267\u884c\u4ee5\u4e0b\u811a\u672c\uff1a cat <<EOF | kubectl -n monitoring apply -f - kind: Service apiVersion: v1 metadata: labels: k8s-app: external-nodes name: external-nodes spec: type: ClusterIP ports: - name: metrics port: 9100 targetPort: 9100 --- kind: Endpoints apiVersion: v1 metadata: labels: k8s-app: external-nodes name: external-nodes subsets: - addresses: - ip: 10.177.9.11 ports: - name: metrics port: 9100 --- apiVersion: monitoring.coreos.com/v1 kind: ServiceMonitor metadata: labels: k8s-app: external-nodes name: external-nodes spec: endpoints: - interval: 60s port: metrics namespaceSelector: matchNames: - monitoring selector: matchLabels: k8s-app: external-nodes EOF kubectl label servicemonitor external-nodes release=pm -n monitoring \u5982\u679c\u540e\u7eed\u9700\u8981\u589e\u52a0\u76d1\u63a7\u8d44\u6e90\uff0c\u9700\u8981\u4fee\u6539\u5df2\u7ecf\u521b\u5efa\u7684endpoint\uff0c\u767b\u5f55\u9c81\u73ed\u7684Master\u8282\u70b9\uff0c\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff1a kubectl edit ep external-nodes -n monitoring ## \u589e\u52a0\u6216\u4fee\u6539subsets\u5b57\u6bb5\uff0c\u6ce8\u610fyaml\u683c\u5f0f \u767b\u5f55prometheus\u754c\u9762\u67e5\u770b\u65f6\u5019\u6dfb\u52a0\u6210\u529ftarget \uff1a http://10.177.152.168:9090/targets\uff0c\u641c\u7d22IP\uff0c\u72b6\u6001\u4e3aup \u767b\u5f55graph\uff0c\u67e5\u770b\u76d1\u63a7\uff1ahttp://10.177.152.168:9090/graph?g0.expr=crash_check&g0.tab=0&g0.stacked=0&g0.range_input=1h \u8f93\u5165\u76d1\u63a7\u9879\uff1a\u6bd4\u5982crash_check \u652f\u6301label\u67e5\u8be2\uff0c\u53c2\u7167wiki\uff1a PromQL","title":"\u914d\u7f6eprometheus\u91c7\u96c6"},{"location":"#_20","text":"kubectl edit prometheuses.monitoring.coreos.com -n monitoring pm-kube-prometheus-stack-prometheus -o yaml","title":"\u914d\u7f6e\u6570\u636e\u5b58\u653e\u65f6\u95f4"},{"location":"#_21","text":"","title":"\u544a\u8b66"},{"location":"#_22","text":"\u76ee\u524dprometheus\u90e8\u7f72\u65b9\u5f0f\u91c7\u7528operator\u90e8\u7f72\uff0c\u521b\u5efa\u544a\u8b66\u7b56\u7565PrometheusRule cat <<EOF | kubectl -n monitoring apply -f - apiVersion: monitoring.coreos.com/v1 kind: PrometheusRule metadata: labels: app: kube-prometheus-stack release: pm name: test-alert-rules namespace: monitoring spec: groups: - name: test rules: - alert: customize-webhook-rule annotations: summary: summary '{{ $labels.target }}' crash_check description: description '{{ $labels.target }}' crash_check message: message '{{ $labels.target }}' crash_check expr: | crash_check <10 for: 1m labels: severity: critical alertTag: vip EOF \u544a\u8b66\u89c4\u5219\u4e5f\u652f\u6301prometheus\u7684\u5404\u79cdlabel\u4ee5\u53ca\u8868\u8fbe\u5f0f\uff0c\u53c2\u7167wiki\uff1a PromQL \u767b\u5f55prometheus\u754c\u9762\u67e5\u770brule\u662f\u5426\u521b\u5efa http://10.177.152.168:9090/rules","title":"\u914d\u7f6e\u544a\u8b66"},{"location":"#_23","text":"\u5982\u679c\u5bf9\u5df2\u7ecf\u521b\u5efa\u7684\u544a\u8b66\u89c4\u5219\u8fdb\u884c\u4fee\u6539\uff0c\u767b\u5f55kubectl\u547d\u4ee4\u64cd\u4f5ck8s\u7684\u4e2dPrometheusRule\u8d44\u6e90\uff0c\u547d\u540d\u7a7a\u95f4\u4e3amonitoring ## \u83b7\u53d6\u5168\u90e8\u544a\u8b66\u89c4\u5219 kubectl get PrometheusRule -n monitoring ## \u4fee\u6539\u544a\u8b66\u89c4\u5219 kubectl edit PrometheusRule {RuleName} -n monitoring","title":"\u544a\u8b66\u89c4\u5219\u64cd\u4f5c"},{"location":"#_24","text":"\u767b\u5f55alertmanager\u754c\u9762 http://10.177.152.168:9093/#/alerts\u67e5\u770b\u544a\u8b66 \u767b\u5f55elasticsearch\u754c\u9762[http://10.177.152.168:9601\uff0c\u67e5\u770b\u544a\u8b66\u5386\u53f2\uff0cindex\u4e3aalertmanager*","title":"\u67e5\u770b\u544a\u8b66"},{"location":"#webhook","text":"\u76ee\u524d\u9752\u5c9b5\u73af\u5883\u5df2\u7ecf\u914d\u7f6e2\u4e2awebhook\uff0c\u4e00\u4e2a\u5199\u5165elasticsearch\uff0c\u7528\u4e8e\u544a\u8b66\u5386\u53f2\u67e5\u8be2\uff1b\u4e00\u4e2a\u5199\u5165\u98de\u4e66\uff0c\u7528\u4e8e\u8fd0\u7ef4\u4eba\u5458\u5b9e\u65f6\u5904\u7406\u3002 \u914d\u7f6e\u547d\u4ee4\u5982\u4e0b kubectl create deploy alertmanager-warning --image=harbor.inner.galaxy.ksyun.com/luban/alertmanager-output -n monitoring kubectl create deploy alertmanager-feishu --image=harbor.inner.galaxy.ksyun.com/luban/feishu-webhook:latest -n monitoring kubectl expose deploy/alertmanager-warning -n monitoring --port=8080 --target-port=8080 kubectl expose deploy/alertmanager-feishu -n monitoring --port=8080 --target-port=8080 cat <<EOF | kubectl -n monitoring apply -f - apiVersion: monitoring.coreos.com/v1alpha1 kind: AlertmanagerConfig metadata: name: warning labels: alertmanagerConfig: warning release: pm spec: route: groupBy: ['alertname'] groupWait: 30s groupInterval: 1m repeatInterval: 1m receiver: 'warning-hook' receivers: - name: 'warning-hook' webhookConfigs: - url: 'http://alertmanager-warning:8080/webhook' --- apiVersion: monitoring.coreos.com/v1alpha1 kind: AlertmanagerConfig metadata: name: feishu labels: alertmanagerConfig: feshu release: pm spec: route: groupBy: ['alertname'] groupWait: 30s groupInterval: 1m repeatInterval: 1m receiver: 'feishu-hook' receivers: - name: 'feishu-hook' webhookConfigs: - url: 'http://alertmanager-feishu:8080/alertmanager-alert' EOF","title":"\u914d\u7f6eWebHook"},{"location":"#_25","text":"","title":"\u65e5\u5fd7"},{"location":"#filebeat","text":"\u4fee\u6539/etc/filebeat/filebeat.yml\uff0c\u6839\u636e\u4e0d\u540c\u65e5\u5fd7\uff0c\u4f20\u5165kafka\u4e0d\u540ctopic\u4e2d logging.level: debug filebeat.inputs: # \u4ece\u8fd9\u91cc\u5f00\u59cb\u5b9a\u4e49\u6bcf\u4e2a\u65e5\u5fd7\u7684\u8def\u5f84\u3001\u7c7b\u578b\u3001\u6536\u96c6\u65b9\u5f0f\u7b49\u4fe1\u606f - type: log # \u6307\u5b9a\u6536\u96c6\u7684\u7c7b\u578b\u4e3a log paths: - /var/log/secure # \u8bbe\u7f6e access.log \u7684\u8def\u5f84 fields: # \u8bbe\u7f6e\u4e00\u4e2a fields\uff0c\u7528\u4e8e\u6807\u8bb0\u8fd9\u4e2a\u65e5\u5fd7 log_topic: topic-for-secure # \u4e3a fields \u8bbe\u7f6e\u4e00\u4e2a\u5173\u952e\u5b57 topic\uff0c\u503c\u4e3a kafka \u4e2d\u5df2\u7ecf\u8bbe\u7f6e\u597d\u7684 topic \u540d\u79f0 - type: log paths: - /var/log/messages # \u8bbe\u7f6e info.log \u7684\u8def\u5f84 fields: # \u8bbe\u7f6e\u4e00\u4e2a fields\uff0c\u7528\u4e8e\u6807\u8bb0\u8fd9\u4e2a\u65e5\u5fd7 log_topic: topic-for-messages # \u4e3a fields \u8bbe\u7f6e\u4e00\u4e2a\u5173\u952e\u5b57 topic\uff0c\u503c\u4e3a kafka \u4e2d\u5df2\u7ecf\u8bbe\u7f6e\u597d\u7684 topic \u540d\u79f0 output.kafka: # \u662f\u5426\u542f\u52a8 enable: true hosts: [\"10.177.152.168:30092\",\"10.177.152.168:30093\",\"10.177.152.168:30094\"] partition.round_robin: #\u5f00\u542fkafka\u7684partition\u5206\u533a reachable_only: true worker: 2 # \u4ee3\u7406\u8981\u6c42\u7684ACK\u53ef\u9760\u6027\u7ea7\u522b # 0=\u65e0\u54cd\u5e94\uff0c1=\u7b49\u5f85\u672c\u5730\u63d0\u4ea4\uff0c-1=\u7b49\u5f85\u6240\u6709\u526f\u672c\u63d0\u4ea4 # \u9ed8\u8ba4\u503c\u662f1 # \u6ce8\u610f:\u5982\u679c\u8bbe\u7f6e\u4e3a0,Kafka\u4e0d\u4f1a\u8fd4\u56de\u4efb\u4f55ack\u3002\u51fa\u9519\u65f6\uff0c\u6d88\u606f\u53ef\u80fd\u4f1a\u6084\u65e0\u58f0\u606f\u5730\u4e22\u5931\u3002 required_acks: 1 compression: gzip #\u538b\u7f29\u683c\u5f0f max_message_bytes: 10000000 #\u538b\u7f29\u683c\u5f0f\u5b57\u8282\u5927\u5c0f # topic: '%{[fields.topic]}' # \u6839\u636e\u6bcf\u4e2a\u65e5\u5fd7\u8bbe\u7f6e\u7684 fields.topic \u6765\u8f93\u51fa\u5230\u4e0d\u540c\u7684 topic topic: '%{[fields.log_topic]}'","title":"\u914d\u7f6efilebeat"},{"location":"#logstash_1","text":"\u5728logging\u7684\u5b89\u88c5\u76ee\u5f55\uff0c\u6839\u636e\u4e0d\u540c\u65e5\u5fd7\uff0c\u83b7\u53d6kafka\u4e0d\u540ctopic\u4e2d,\u4fee\u6539values.yaml logstashConfig: logstash.yml: | http.host: 0.0.0.0 monitoring.elasticsearch.hosts: http://elasticsearch-master:9200 # Allows you to add any pipeline files in /usr/share/logstash/pipeline/ ### ***warn*** there is a hardcoded logstash.conf in the image, override it first logstashPipeline: logstash.conf: | input { kafka { bootstrap_servers => \"kafka-0.kafka-headless.elastic-system.svc.cluster.local:9092,kafka-1.kafka-headless.elastic-system.svc.cluster.local:9092,kafka-2.kafka-headless.elastic-system.svc.cluster.local:9092\" topics => [\"topic-for-secure\"] } } output { elasticsearch { hosts => [\"http://elasticsearch-master:9200\"] index => \"topic-for-secure-%{+YYYY.MM.dd}\" } } # logstash.conf: | # input { # exec { # command => \"uptime\" # interval => 30 # } # } # output { stdout { } } \u5728k8s\u4e2d\u521b\u5efa\u65b0\u7684logstash kubectl create ns elastic-system export REGISTRY=\"harbor.inner.galaxy.ksyun.com\" helm install {logstash--new-name} ./logstash -n elastic-system --set imageTag=7.10.1 --set image=$REGISTRY/luban/logstash","title":"\u914d\u7f6elogstash"},{"location":"#kibana_1","text":"\u767b\u5f55kibana\u754c\u9762\uff0c\u67e5\u770bes\u4e2d\u662f\u5426\u521b\u5efa\u65b0\u7684index http://10.177.152.168:9601/app/management/data/index_management/indices \u521b\u5efakibana\u7684indexPatterns http://10.177.152.168:9601/app/management/kibana/indexPatterns","title":"\u914d\u7f6ekibana"},{"location":"#_26","text":"\u9700\u8981\u6ce8\u610f\uff0ces\u4e2d\u7684index\u9700\u8981\u6709@timestamp \u5728logging\u7684\u5b89\u88c5\u76ee\u5f55\uff0c\u4fee\u6539 kk get cm -n elastic-system curator-elasticsearch-curator-config -o yaml","title":"\u914d\u7f6e\u65e5\u5fd7\u6e05\u7406"},{"location":"#_27","text":"","title":"\u5b58\u50a8\u5bb9\u91cf\u89c4\u5212"},{"location":"#_28","text":"\u78c1\u76d8\u5bb9\u91cf\u89c4\u5212\uff0c\u9996\u5148\u660e\u786e\u51e0\u4e2a\u6982\u5ff5: \u76d1\u63a7\u8282\u70b9 : \u4e00\u4e2a exporter \u8fdb\u7a0b\u88ab\u8ba4\u4e3a\u662f\u4e00\u4e2a\u76d1\u63a7\u8282\u70b9\u3002Manager \u5728\u5b89\u88c5 AQUILA\u65f6\uff0c\u9ed8\u8ba4\u6bcf\u4e2a\u8282\u70b9\u90fd\u4f1a\u5b89\u88c5\u4e00\u4e2a node-exporter \u6536\u96c6\u8282\u70b9\u4fe1\u606f(CPU, Memory \u7b49), \u6bcf\u4e2a\u8282\u70b9\u5b89\u88c5\u4e00\u4e2a tdh-exporter \u6536\u96c6 TDH Services metrics (\u76ee\u524d\u6709: HDFS, YARN, ZOOKEEPER, KAFKA, HYPERBASE, INCEPTOR). \u6545\u5728 TDH \u96c6\u7fa4\u4e0a, \u6bcf\u4e2a\u8282\u70b9\u6709\u4e24\u4e2a exporter \u8fdb\u7a0b. \u6d4b\u91cf\u70b9 : \u4e00\u4e2a\u6d4b\u91cf\u70b9\u4ee3\u8868\u4e86\u67d0\u76d1\u63a7\u8282\u70b9\u4e0a\u7684\u4e00\u4e2a\u89c2\u6d4b\u5bf9\u8c61. \u4ece\u67d0\u6d4b\u91cf\u70b9\u91c7\u96c6\u5230\u7684\u4e00\u7ec4\u6837\u672c\u6570\u636e\u6784\u6210\u4e00\u6761\u65f6\u95f4\u5e8f\u5217\uff08time series). \u6293\u53d6\u95f4\u9694 : Promtheus \u5bf9\u67d0\u4e2a\u76d1\u63a7\u8282\u70b9\u91c7\u96c6 metrics \u7684\u65f6\u95f4\u95f4\u9694. \u4e00\u822c\u4e3a\u540c\u7c7b\u76d1\u63a7\u8282\u70b9\u8bbe\u7f6e\u76f8\u540c\u7684\u6293\u53d6\u95f4\u9694. AQUILA\u5bf9\u5e94\u7684\u914d\u7f6e\u503c\u4e3a: prometheus.node.exporter.scrape_interval(\u9ed8\u8ba415s) \u548c prometheus.tdh.exporter.scrape_interval(\u9ed8\u8ba460s) \u4fdd\u7559\u65f6\u95f4 : \u6837\u672c\u6570\u636e\u5728\u78c1\u76d8\u4e0a\u4fdd\u5b58\u7684\u65f6\u95f4,\u8d85\u8fc7\u8be5\u65f6\u95f4\u9650\u5236\u7684\u6570\u636e\u5c31\u4f1a\u88ab\u5220\u9664. \u5b58\u50a8\u5728\u78c1\u76d8\u4e0a\u7684\u6837\u672c\u90fd\u662f\u7ecf\u8fc7\u7f16\u7801\u4e4b\u540e\u7684\u6837\u672c(\u5bf9\u6837\u672c\u8fdb\u884c\u8fc7\u6570\u636e\u7f16\u7801, \u4e00\u822c\u4e3a double-delta \u7f16\u7801). AQUILA\u5bf9\u5e94\u7684\u914d\u7f6e\u503c\u4e3a prometheus.storage.retention.time(\u9ed8\u8ba415\u5929) \u6d3b\u8dc3\u6837\u672c\u7559\u5b58\u65f6\u95f4 : \u7559\u5b58\u4e8e\u5185\u5b58\u7684\u6d3b\u8dc3\u6837\u672c\uff08\u5df2\u7ecf\u88ab\u7f16\u7801\uff09\u5728\u5185\u5b58\u4fdd\u7559\u65f6\u95f4. \u5728\u5185\u5b58\u4e2d\u7684\u7559\u5b58\u6570\u636e\u8d8a\u591a\uff0c\u67e5\u8be2\u8fc7\u5f80\u6570\u636e\u7684\u6027\u80fd\u8d8a\u9ad8\uff0c\u4f46\u662f\u6d88\u8017\u5185\u5b58\u4e5f\u4f1a\u589e\u52a0. \u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u9700\u8981\u6839\u636e\u6240\u76d1\u63a7\u7684\u4e1a\u52a1\u7684\u6027\u8d28\uff0c\u8bbe\u5b9a\u5408\u7406\u7684\u5185\u5b58\u7559\u5b58\u65f6\u95f4. AQUILA\u5bf9\u5e94\u7684\u914d\u7f6e\u503c\u4e3a prometheus.min-block-duration (\u9ed8\u8ba42h), prometheus.max-block-duration(\u9ed8\u8ba426h). Facebook \u5728\u8bba\u6587 \u300aGorilla: A Fast, Scalable, In-Memory Time Series DataBase\u300b \uff08Prometheus\u5b9e\u73b0\u53c2\u8003\u8bba\u6587\uff09\u4e2d\u7ed9\u51fa\u4e86\u7559\u5b58\u5185\u5b58\u65f6\u95f4\u7684\u4e00\u822c\u7ecf\u9a8c: 26 h. \u6837\u672c(\u6d4b\u91cf\u70b9)\u5927\u5c0f : \u6839\u636e Prometheus \u5b98\u65b9\u6587\u6863\u8bf4\u660e, \u6bcf\u4e00\u4e2a\u7f16\u7801\u540e\u7684\u6837\u672c\u5927\u6982\u5360\u75281-2\u5b57\u8282\u5927\u5c0f","title":"\u76d1\u63a7"},{"location":"#_29","text":"\u9700\u8981\u6839\u636e\u4e0d\u540c\u7684\u65e5\u5fd7\uff0c\u4ee5\u53ca\u65e5\u5fd7\u7684\u5927\u5c0f\u89c4\u5212 \u767b\u5f55kibana\u67e5\u770bindex\u4f7f\u7528\u5927\u5c0f\uff0c\u89c4\u5212\u65e5\u5fd7\u7684\u5b58\u50a8 http://10.177.152.168:9601/app/management/data/index_management/indices","title":"\u65e5\u5fd7"},{"location":"#_30","text":"","title":"\u9644\u5f55"},{"location":"#1hosts","text":"https://wiki.op.ksyun.com/pages/viewpage.action?pageId=157002294","title":"\u9752\u5c9b1\u53d1\u5e03\u73af\u5883hosts\u4fe1\u606f"},{"location":"#_31","text":"Prometheus Server\u76d1\u63a7\u76ee\u6807\u4e3b\u673a\u4e0a\u66b4\u9732\u7684http\u63a5\u53e3\uff08\u8fd9\u91cc\u5047\u8bbe\u63a5\u53e3A\uff09\uff0c\u901a\u8fc7\u4e0a\u8ff0Promethes\u914d\u7f6e\u7684'scrape_interval'\u5b9a\u4e49\u7684\u65f6\u95f4\u95f4\u9694\uff0c\u5b9a\u671f\u91c7\u96c6\u76ee\u6807\u4e3b\u673a\u4e0a\u76d1\u63a7\u6570\u636e\u3002 \u5f53\u63a5\u53e3A\u4e0d\u53ef\u7528\u7684\u65f6\u5019\uff0cServer\u7aef\u4f1a\u6301\u7eed\u7684\u5c1d\u8bd5\u4ece\u63a5\u53e3\u4e2d\u53d6\u6570\u636e\uff0c\u76f4\u5230\"scrape_timeout\"\u65f6\u95f4\u540e\u505c\u6b62\u5c1d\u8bd5\u3002\u8fd9\u65f6\u5019\u628a\u63a5\u53e3\u7684\u72b6\u6001\u53d8\u4e3a\u201cDOWN\u201d\u3002 Prometheus\u540c\u65f6\u6839\u636e\u914d\u7f6e\u7684\"evaluation_interval\"\u7684\u65f6\u95f4\u95f4\u9694\uff0c\u5b9a\u671f\uff08\u9ed8\u8ba41min\uff09\u7684\u5bf9Alert Rule\u8fdb\u884c\u8bc4\u4f30\uff1b\u5f53\u5230\u8fbe\u8bc4\u4f30\u5468\u671f\u7684\u65f6\u5019\uff0c\u53d1\u73b0\u63a5\u53e3A\u4e3aDOWN\uff0c\u5373UP=0\u4e3a\u771f\uff0c\u6fc0\u6d3bAlert\uff0c\u8fdb\u5165\u201cPENDING\u201d\u72b6\u6001\uff0c\u5e76\u8bb0\u5f55\u5f53\u524dactive\u7684\u65f6\u95f4\uff1b \u5f53\u4e0b\u4e00\u4e2aalert rule\u7684\u8bc4\u4f30\u5468\u671f\u5230\u6765\u7684\u65f6\u5019\uff0c\u53d1\u73b0UP=0\u7ee7\u7eed\u4e3a\u771f\uff0c\u7136\u540e\u5224\u65ad\u8b66\u62a5Active\u7684\u65f6\u95f4\u662f\u5426\u5df2\u7ecf\u8d85\u51farule\u91cc\u7684\u2018for\u2019 \u6301\u7eed\u65f6\u95f4\uff0c\u5982\u679c\u672a\u8d85\u51fa\uff0c\u5219\u8fdb\u5165\u4e0b\u4e00\u4e2a\u8bc4\u4f30\u5468\u671f\uff1b\u5982\u679c\u65f6\u95f4\u8d85\u51fa\uff0c\u5219alert\u7684\u72b6\u6001\u53d8\u4e3a\u201cFIRING\u201d\uff1b\u540c\u65f6\u8c03\u7528Alertmanager\u63a5\u53e3\uff0c\u53d1\u9001\u76f8\u5173\u62a5\u8b66\u6570\u636e\u3002 AlertManager\u6536\u5230\u62a5\u8b66\u6570\u636e\u540e\uff0c\u4f1a\u5c06\u8b66\u62a5\u4fe1\u606f\u8fdb\u884c\u5206\u7ec4\uff0c\u7136\u540e\u6839\u636ealertmanager\u914d\u7f6e\u7684\u201cgroup_wait\u201d\u65f6\u95f4\u5148\u8fdb\u884c\u7b49\u5f85\u3002\u7b49wait\u65f6\u95f4\u8fc7\u540e\u518d\u53d1\u9001\u62a5\u8b66\u4fe1\u606f\u3002 \u5c5e\u4e8e\u540c\u4e00\u4e2aAlert Group\u7684\u8b66\u62a5\uff0c\u5728\u7b49\u5f85\u7684\u8fc7\u7a0b\u4e2d\u53ef\u80fd\u8fdb\u5165\u65b0\u7684alert\uff0c\u5982\u679c\u4e4b\u524d\u7684\u62a5\u8b66\u5df2\u7ecf\u6210\u529f\u53d1\u51fa\uff0c\u90a3\u4e48\u95f4\u9694\u201cgroup_interval\u201d\u7684\u65f6\u95f4\u95f4\u9694\u540e\u518d\u91cd\u65b0\u53d1\u9001\u62a5\u8b66\u4fe1\u606f\u3002\u6bd4\u5982\u914d\u7f6e\u7684\u662f\u90ae\u4ef6\u62a5\u8b66\uff0c\u90a3\u4e48\u540c\u5c5e\u4e00\u4e2agroup\u7684\u62a5\u8b66\u4fe1\u606f\u4f1a\u6c47\u603b\u5728\u4e00\u4e2a\u90ae\u4ef6\u91cc\u8fdb\u884c\u53d1\u9001\u3002 \u5982\u679cAlert Group\u91cc\u7684\u8b66\u62a5\u4e00\u76f4\u6ca1\u53d1\u751f\u53d8\u5316\u5e76\u4e14\u5df2\u7ecf\u6210\u529f\u53d1\u9001\uff0c\u7b49\u5f85\u2018repeat_interval\u2019\u65f6\u95f4\u95f4\u9694\u4e4b\u540e\u518d\u91cd\u590d\u53d1\u9001\u76f8\u540c\u7684\u62a5\u8b66\u90ae\u4ef6\uff1b\u5982\u679c\u4e4b\u524d\u7684\u8b66\u62a5\u6ca1\u6709\u6210\u529f\u53d1\u9001\uff0c\u5219\u76f8\u5f53\u4e8e\u89e6\u53d1\u7b2c6\u6761\u6761\u4ef6\uff0c\u5219\u9700\u8981\u7b49\u5f85group_interval\u65f6\u95f4\u95f4\u9694\u540e\u91cd\u590d\u53d1\u9001\u3002 \u540c\u65f6\u6700\u540e\u81f3\u4e8e\u8b66\u62a5\u4fe1\u606f\u5177\u4f53\u53d1\u7ed9\u8c01\uff0c\u6ee1\u8db3\u4ec0\u4e48\u6837\u7684\u6761\u4ef6\u4e0b\u6307\u5b9a\u8b66\u62a5\u63a5\u6536\u4eba\uff0c\u8bbe\u7f6e\u4e0d\u540c\u62a5\u8b66\u53d1\u9001\u9891\u7387\uff0c\u8fd9\u91cc\u6709alertmanager\u7684route\u8def\u7531\u89c4\u5219\u8fdb\u884c\u914d\u7f6e\u3002","title":"\u544a\u8b66\u4ea7\u751f\u6d41\u7a0b"}]}